<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<title>Constitutive Practices of Administrative Law as Limits on Legal Automation</title>
<style>
    body {font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; background: #fafafa; color: #222;}
    h1, h2, h3, h4 {color: #0f172a;}
    p {margin: 0.6em 0;}
   details {
   border: 1px solid #ddd;
   border-radius: 8px;
   padding: 1px 5px; 
   margin: 0px 0px;
   background: #f9f9f9;
   font-size: 15px; 
   }
  summary {
  display: block;       
  padding: 6px 6px;
  font-weight: bold;
  font-size: 16px; 
  cursor: pointer;
  color: Green;
  background: #fff;
  border-bottom: 1px solid #ddd;
}
  .container {
  max-width: 1000px; /* 原来是 700px */
  margin: 3rem auto;
  padding: 0 1rem;
}

    .toc {background:#fff; padding:1em; border:1px solid #ddd; border-radius:8px; margin-bottom:1.5em;}
    .toc ul {list-style:none; padding-left:0;}
    .toc li {margin:0.3em 0;}
    .toc a {text-decoration:none; color:#0ea5a4;}
    .toc a:hover {text-decoration:underline;}
    </style>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>Constitutive Practices of Administrative Law as Limits on Legal Automation</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <main class="container">
    <h1>Artificial Intelligence and the Rule of Law</h1>
    <p>作者：Aziz Z. Huq</p>
    <p>引用：Aziz Z. Huq, <i>Artificial Intelligence and the Rule of Law</i>, Public Law and Legal Theory Working Paper Series, No. 764 (2021).</p>
         <hr />
<body>
  <article>

    <h2>🗂️大纲</h2>

    <h2>📝翻译</h2>

    <h3>引言</h3>
    <p>本章探讨了技术冲击与“法治”之间的相互作用。为实现这一目的，本章分析了一类关联松散的计算技术——即被称为“机器学习”（ML）或（更不精确的说法）“人工智能”（AI）——所带来的影响。目前，（1）这些工具被应用于执法的预决阶段，例如协助筛选税务和监管调查的目标（Coglianese and Lehr, 2016）。（2）它们也越来越多地被用于裁判adjudication过程中，例如在审前保释裁定中协助并指导对个人暴力风险的判定（Huq, 2019）。关于“代码驱动的判断将全面取代人类判断”的预测层出不穷（Re and Solow-Niedemann, 2019; Volokh, 2019; but see Wu, 2019）。但几乎同样多的声音，也在强烈谴责这一可能性。对审判系统的公平性、透明度和公正性的预期影响，是这类反对意见的主要依据（Michaels, 2019; O’Neil, 2016）。即便这些批评未明确以“法治”为框架展开，它们也往往与通常在“法治”范畴下讨论的规范性关切存在重叠或紧密关联。（<span style="color:blue;">这些对算法的批评其理论根源在“法治”的规范性要求方面。</span>）</p>

    <p>两个与法治相关的一般性问题由这些发展产生。</p>

    <p>【问题一：技术与现有法治理念的相容性】</p>

    <p>更显而易见的问题是，这些技术融入法律体系后，其本身是否与法治相容或冲突。根据所采用的法治概念不同，用机器决策取代人类判断可能会引发基于透明度、可预测性、偏见和程序公正的反对意见。本章的首要目的是探讨这种技术冲击如何带来此类挑战。笔者认为，法治的规范性目标与机器学习（ML）技术之间的相互作用复杂且模糊。此外，在许多情况下，对技术更有力的规范性反对，并非源于其被采用这一单纯事实，而是源于技术采用所处的社会政治背景，以及技术对权力和资源背景差异的动态影响。（<span style="color:blue;">脱离当事人资源等现实社会背景/场景，形而上地讨论使用算法这一单纯事实对法治的影响是无力的。直接讨论算法和法治很牵强，算法通过影响社会权力等背景因素与法治发生关联。</span>）机器学习的采用可能会加剧社会权力和地位的差异，从而使法治面临压力。（<span style="color:green;">分析的路径：算法——对权力、资源等现实的影响——对现有法治理论的前提假设产生了哪些改变——与现有法治理论的相容性。</span>）关注这种动态，有助于将目光投向法治理论中已被注意到（如Gowder, 2016; Wilmot-Smith, 2019）但未被深入研究的一个主题：即社会经济动态与法治之间的相互作用。（<span style="color:green;">启示：在讨论时可能需要对场景更细致的分类，分类标准需要涵盖当事人的资源，包括已有的和投入的意愿，进行更加细致的成本收益分析。</span>）</p>

    <p>【问题二：技术对法治概念和实现方式的影响】</p>

    <p>第二个由人工智能（AI）和机器学习新技术引发的问题也未得到广泛讨论，但或许具有更深远的意义。该问题不再聚焦于新技术是否符合法治价值，而是围绕机器学习和人工智能技术对法治本身的概念界定或实施方式所产生的影响。正如Taekema (2020) 最近所指出的，许多关于法治的经典论述——包括Dicey和Waldron的论述——都将概念定义与一系列制度性必然结果交织在一起。例如，Fuller (1964)、Raz (1979) 和Waldron (2011) 均认为，法治需要或多或少特定的制度形式，包括法院。他们大概还认为，人类法官行使自由裁量权并作出判断是必要的，而非可选择的。对于法治的这些制度性必然结果而言，用机器学习技术取代人类很可能产生不稳定的影响。这使得“法治这一抽象概念是否需要通过特定制度形式来实现”这一问题更加尖锐，也引发了一个疑问：技术变革是否可能要求调整法治概念与实践之间的关系？因为在技术变革的条件下，既有的规范性概念及其实际的制度关联可能不再成立。至少，在这种条件下，明确法治的制度形式不仅在实践层面，也在法律理论层面带来了挑战。（<span style="color:green;">现有的法律制度设计被认为是法治要求的必然结果，但这种关联性可能得到质疑。AI的出现给了一个重新思考这种关联性的机会，哪些制度设计是必然的，哪些制度设计是工具性的。</span>）</p>

    <p>本章首先简要介绍机器学习目前及未来可能在法律执行和审判中的应用，并指出适用于这些场景的不同考量因素（在本章后续内容中，笔者将聚焦机器学习并使用“ML”这一术语，因其是最相关的新型计算技术，而“AI”一词过于宽泛，可能造成混淆）。随后，笔者将探讨这种技术变革对法治价值带来的挑战，尤其会分析机器学习作为新技术与社会经济安排的相互作用，以及这种动态关系对法治的影响。最后，笔者将思考机器学习的出现是否应促使人们重新思考法治的概念界定和实现方式，以及对法治的抽象思考与实践思考能否清晰分割。</p>

    <h3>一、法律系统中机器学习的使用</h3>

    <h4>【ML技术介绍】</h4>
    <p>总体而言，机器学习（ML）算法旨在解决一个“学习问题……即通过某种类型的训练经验，提升执行某项任务时的某种性能指标”（Jordan and Mitchell, 2015, p.255）。因此，它能“从数据中学习规则”（Obermeyer and Emanuel, 2016, p.1217）。这一任务类似于普通最小二乘法、逻辑回归等常见工具的功能——这些工具从现有数据中推导出描述不同参数间关系的方程，该方程可应用于原始数据样本之外，实质是进行预测。通常情况下，机器学习工具执行此类预测任务时，产出结果的偏差和方差低于常见的回归工具。但机器学习模型的复杂度并不能可靠地反映其质量：在某些场景中，相对简单的计算模型可能比复杂模型产出更准确的结果（Jung et al., 2020）。</p>

    <p>机器学习工具的分类方式多样。例如，Domingos (2015) 将机器学习研究划分为五个“流派”，每个流派都有其深层理论基础。从更实用的角度看，机器学习工具中最显著的分类是有监督工具与无监督工具的区别。有监督机器学习算法会定义一个函数f(x)，为任意给定输入x生成输出y，其输出形式即把x归入不同的y类别。例如，它可将图像分为“人脸”与“非人脸”，将嫌疑人分为“危险”与“非危险”，或分为“猫”“狗”“老鼠”等类别（Alpaydin, 2020）。相比之下，无监督机器学习算法以未标记的训练数据为起点，仅根据数据的内在结构构建分类，而非依赖程序员的任何预先指导（Kelleher and Tierney, 2018; Flasch, 2012）。例如，给定一组网络图像，无监督算法可能将其归入若干预先未指定的类别，如“猫与非猫”“人物与物体”等。这些类别可理解为数据中彼此相似度高于与其他实例的实例集群：算法通过构建多层表征（每层表征的抽象程度不同）识别这些集群，并通过迭代更新数据中不同集群的边界，提高集群内相似度与集群间差异度。通俗而言，无监督机器学习的目标是“了解通常会发生什么、不会发生什么”（Alpaydin, 2020, p.11; Kelleher and Tierney, 2018）。</p>

    <p>有监督和无监督机器学习（ML）工具具有若干共同特征。第一，两者均依赖一组“训练数据”，通过分析这些数据来评估不同变量之间的关系。在某些情况下，这些数据是历史数据，例如过往的医疗记录、某城市的历史犯罪数据，或是特定人群输入的互联网搜索列表。第二，机器学习工具的任务是构建一个模型，该模型可根据一组输入变量估算结果变量。在构建模型的过程中，工具需遵循一个成本函数（有时也称为奖励函数），该函数会定义机器应进行的推理类型。最终形成的模型既可以是预测性的——即对未发生的事件进行推理，也可以是描述性的——即让人类注意到原本可能被忽视的相关性或关系。第三，该模型会被应用于新的、不属于训练数据集的“样本外”数据，这正是“学习”能力的核心应用体现。</p>

    <h4>【私营企业广泛应用】</h4>
    <p>机器学习（ML）工具已被私营企业广泛采用。网飞（Netflix）和亚马逊（Amazon）等公司使用的推荐系统是常见且为人熟知的例子，但并非唯一案例。谷歌（Google）的网页排名（Pagerank）算法，以及数百万社交媒体用户浏览的脸书（Facebook）动态推送，均依赖机器学习工具。在分析这些应用时，Kelleher和Tierney（2018, pp.151-80）将适合用机器学习解决的“现实世界”问题归纳为四大类：群体内集群或关联的识别、群体内异常值的识别、关联规则的构建，以及分类与回归预测问题。相比之下，机器学习工具在因果估算方面的应用则较为有限。（机器学习不太擅长因果关系推断。但现在的gpt推理是否是因果关系？需要研究）</p>

    <h4>【政府的应用】</h4>
    <p>政府是较晚采用机器学习工具的使用者，但其态度积极且迫切。就当前研究目的而言，有必要区分政府的两类使用场景——它们与法治问题尤为相关。</p>

    <p><b>第一类（也是更常见的一类）是将机器学习应用于调查和目标筛选阶段；</b></p>

    <p><b>第二类（目前应用较少）是在审判中替代人类法官。</b></p>

    <p>当然，这两类应用仅是政府使用机器学习工具的一部分。采用计算机视觉或音频识别技术的国防科技、使用机器学习诊断工具的公立医院，以及借助谷歌（Google）开展工作的政府雇员，均属于“政府机器学习应用”，但不在下文以审判为核心的讨论范围内。</p>

    <p>为使理论分析更易处理，不妨先详细阐述这两类常见应用场景。</p>

    <p>首先，政府可能利用机器学习工具决定调查对象，或分配稀缺资源（无论是援助资源还是干预资源）。例如，一项针对美国政府采用机器学习工具的近期研究发现，联邦政府民用部门中约有64个不同机构使用着157种“人工智能/机器学习”工具。其中许多应用涉及分析自报数据或公开数据，以识别潜在的违法行为。以美国税务机构国税局（Internal Revenue Service）为例，其通过机器学习工具分析大量纳税申报文件，识别具有欺诈迹象的申报单（Engstrom et al., 2020）。此类工具并非联邦政府的专属：许多城市使用机器学习工具分配消防检查员，或确定需检查卫生违规情况的餐厅（Athey, 2017）。当私营企业采用机器学习流程时，政府可要求其设置特定功能，以便利自身监督企业是否符合法律标准（Morse, 2019）。从洛杉矶开始，美国已有60多个警察部门采用了“Predpol”——这是一款颇具争议的预测工具，旨在挖掘历史犯罪数据，识别次日可能发生犯罪的区域（Huq, 2019）。提供福利及其他服务的州和地方机构，也正借助机器学习工具对服务对象进行分类。无论是提供（收容所等）积极干预，还是开展（虐待儿童调查等）消极干预，机器学习工具都能提供协助（Eubanks, 2018）。</p>

    <p>……</p>

    <p>其次，政府也可在审判场景中使用机器学习（ML）工具，作为人类判断的部分替代。迄今为止，这类应用的频率低于调查阶段的应用。最具代表性的案例目前仍处于提议阶段，即利用机器学习工具预测刑事被告在审判前是否可能实施暴力行为。其学术支持者认为，这一做法有望降低审前拘留率，同时更大程度保障公共安全（Kleinberg et al., 2015）。</p>

    <p>关于机器学习在审判中对人类角色的替代程度，目前存在争议。</p>

    <ul>
      <li>Wu (2019) 认为，机器学习完全取代人类法官的可能性不大。他指出，审判案件包括“简单案件”和“疑难案件”两类：机器学习工具虽对简单案件有用，但面对新的事实组合时，可能会产生“危险或荒谬”的结果。因此，Wu 预测将出现“半机械人（cyborg）系统”，即“将规模和效率与人类对疑难案件的审判相结合”。</li>
      <li>Volokh (2019) 则更为乐观，他认为一旦解决了“设计能生成有说服力法律文本的人工智能”这一技术问题，就“几乎没有概念上的理由”去反对将该技术应用于法官的角色。</li>
    </ul>

    <h4>【糟糕的案例】</h4>
    <p>就当前研究目的而言，只需明确一点：Wu 所预测的“半机械人系统”已实际存在，其特征是机器学习在“简单案件”中的应用占绝对数量优势，而人类对“疑难案件”的判断则极为罕见。以下举两个例子说明。第一个例子是Compas——这是一种非机器学习算法，目前在美国多个司法管辖区用于指导法官的保释裁定：它会为被告生成1至10分的风险评分，为负责决定是否准予保释的治安法官提供参考，进而影响被告在刑事审判前能否及如何获得释放（Huq, 2019）。Compas的使用极具争议，尤其是长期面临“存在种族偏见”的批评。具体而言，批评者指出，根据其预测结果，事实上无罪却被不当拘留的黑人被告比例，高于事实上无罪却被不当拘留的白人被告比例。</p>

    <p>第二个例子是2013年，密歇根州州长里克·斯奈德（Rick Snyder）推出了一款名为Midas的算法工具，用于检测失业救济金申请中的欺诈行为——这是该州信息技术全面改革的一部分（Charette, 2018）。Midas系统的拒绝率高达93%，同时错误地将4万名密歇根州居民判定为欺诈性申领救济金。该系统还缺乏让个人对救济金拒绝决定提出异议的机制。在新冠疫情引发失业申请激增前，该州福利机构仅雇佣12人处理和纠正欺诈指控。即便疫情加剧了申请压力，拨打该机构电话的申请人也无法接通州政府工作人员，而是被转接到其他被拒绝申领的申请人。据称，被错误拒绝救济金的申请人每天拨打州政府办公室电话逾千次，仍无法接通。该州还选择提供“提交信息机会相对有限、事后修改或更正机会也相对较少”的用户界面。无论该工具的预测结果在形式上是“仅供参考”还是“推定有效”，密歇根州这一算法的制度背景使其预测结果对数万人产生了事实上的约束力。这几乎注定了其在欺诈检测中的极高假阳性率（Cuéllar and Huq, forthcoming 2021）。（1）Midas系统很好地说明了为何即便“半机械人系统”也会引发与法治相关的担忧：“简单案件”在数量上占主导，而“疑难案件”面临极高的诉讼障碍。因此，在实践中，Midas系统主导了决策空间——除非对整个系统提起法律质疑，否则人类判断几乎没有发挥空间。（2）该系统还表明，“执法自动化”与“审判自动化”之间的界限可能并不清晰：在许多场景中，执法裁量权的自动化会排挤审判监督的可能性。像Midas生成的这类机器决策可能难以理解，因此也难以提出异议。</p>

    <p>政府对机器学习（ML）的采用很可能会加速。Huq和Cuéllar（forthcoming 2021）认为，政府对机器学习的采用发生在一个动态的双层背景下。根据这一基于政治经济学的观点，政府处于两个动态环境中运作：一是国内政治环境，由竞相扩大机器学习能力并将其货币化的企业主导；二是国际环境，政府需与其他主权国家竞争——这些国家正为地缘战略目的培育和部署相同的技术能力。机器学习工具如何部署、出于何种目的部署，取决于国家政府在这两个相互重叠但又截然不同的环境中所做的战略选择。Huq和Cuéllar总结道，采用机器学习的压力不仅取决于法律，还取决于地缘政治环境与国内利益集团之间的动态相互作用。因此，我们不仅需要思考机器学习目前的应用，还应展望未来——即新的应用可能会越来越多地将人类判断从审判过程中取代。（<span style="color:blue;">与此相关的研究应该是面向未来的，不仅局限于现有的实践。思考未来更高程度替代后的法律问题与制度设计</span>）</p>

    <h3>二、机器学习作为一种对法治的减损</h3>

    <p>机器学习（ML）对审判的潜在自动化，给法治价值带来了挑战与机遇。要深入分析这两方面，显然需要先从法治的定义入手。当然，关于法治的定义存在广泛分歧，但对法治概念进行一个粗略（且刻意追求无争议的通俗）分类后会发现：无论采用何种定义，用机器决策取代人类审判判断，都不太可能毫无问题。</p>

    <h4>【法治的概念/界定】</h4>
    <p>法治大致可分为形式性、实质性和程序性三种形态。形式与实质的区分是学界熟知的（Tamanaha, 2004, pp.91-92），而程序性类别的加入，则体现了近期研究中“自觉从更侧重制度的视角界定法治”的思路。</p>

    <ol>
      <li>朗·富勒（Lon Fuller, 1964）提出的著名法治价值清单——法律必须具有一般性、公开性、前瞻性、明确性、一致性、稳定性、可遵守性，并由官员予以执行——常被归为“形式性”法治（Craig, 1997; Tamanaha, 2004）。但加德纳（Gardner, 2012, pp.199-204）指出，富勒的这些标准“对法律内容作出了评判”，且很容易衍生出可依法执行的权利。他认为，富勒的清单更应被理解为“关于‘如何做’的道德，而非‘为何做’的道德”（同前引，第206页），其核心关注法律的“生成与适用方式”（Raz, 2019, p.2）。本文将用“形式性法治”这一概念来概括这一内涵。</li>
      <li>对法治的第二种理解指向“为何做”，因此可被称为“实质性”法治。对此最简洁的阐述或许来自宾厄姆勋爵（Lord Bingham）的观点：要实现法治，“法律必须为基本人权提供充分保护”（Bingham, 2011, p.75）。类似地，T.R.S. 艾伦（T.R.S. Allen）将法治描述为“一套基本原则和价值的集合，这些原则和价值共同为法律秩序赋予了一定的稳定性和连贯性”，并将“关于个人自由和自然正义的传统理念，以及更宽泛意义上政府与民众关系中的个人正义与公平理念”纳入其中（Allen, 1993, pp.21-22）。</li>
      <li>最后，沃尔德伦（Waldron）将法治与程序/过程性特质相关联，例如在公正法庭前获得听证的权利、提出证据和法律论证的权利，以及获得决策理由说明的权利。他认为这些权利对维护人的尊严至关重要，而人的尊严意味着个体有权被视为“具有主动思考能力的智慧体”（Waldron, 2011, p.23；另见Waldron, 2008）。这种程序性法治观，或许可被理解为已融入某些形式性法治的定义中——例如，沃尔德伦的定义可被解读为对富勒形式性法治定义中第八项（也是最后一项）要素的细化，该项要素要求官员执行已颁布的规则。但沃尔德伦的论述内容足够丰富，且在语气和细节上与富勒的简洁主张有显著区别，因此将其视为一个独立的观点是合理的。与富勒的论述不同，沃尔德伦的核心在于接纳并致力于促进“个体的尊严与能动性——他们拥有自己的观点和值得倾听的论证”，而这种核心主张是法治的实质性或形式性表述均无法实现的（Taekema, 2013, p.145）。</li>
    </ol>

    <h4>【本文对上述三种概念与算法关系的理解和聚焦】</h4>
    <p>上述每一种法治理论，都以不同方式与机器学习（ML）和人工智能（AI）的兴起产生互动。追溯这些互动关系，既能阐明这些技术涉及的规范性风险，也有助于厘清技术对法治最重要的影响——即技术会维持并加剧影响力、特权和地位方面的背景不平等。本文将聚焦法治的形式性与程序性理解。由于实质性法治理论需要明确“基本”或“根本”权利的内涵，而这与法律“如何”适用的问题相区分，因此其与自动化决策流程的相容性具有偶然性。诚然，对自动化审判系统存在一系列异议，这些异议最好从基本的实质性道德原则（而非公正性或中立性）角度理解。例如，对Compas系统的一项批评指出，其预测基于反映并融入偏见假设的数据（如通过歧视性执法收集的数据），且为“危险性”概念赋予了虚假的科学有效性——而该概念在经验数据中并无可靠依据（Koepke & Robinson, 2018）。依赖反映种族或性别分层历史模式的训练数据，也可能导致输出结果延续这些模式（Huq, 2019）。尽管这一异议很重要，但并非机器学习语境下所特有，而是适用于任何“以受历史偏见扭曲的数据作为未来资源分配依据”的情况（例如劳动力市场中教育回报的分配）。但抛开这些普遍性异议，我们没有理由认为，实质性法治必然会与“使用机器学习工具而非人类决策者”的选择产生冲突。</p>

    <h4>【ML与形式性法治的互动】</h4>
    <p>首先来看机器学习工具与富勒（Fuller）所定义的形式性法治的互动关系。根据自动化系统的实施方式不同，其对法治的影响可能是有益的，也可能是有害的。</p>

    <p><b>算法作为法律，在相对人可理解方面存在缺陷</b></p>
    <p>富勒指出，法治失效的情形之一是“缺乏公开的法典来宣告未来争议中适用的规则”，且该法典必须“能被普通公民或受过训练的律师理解”（Fuller, 1964, pp.35-36）。立法晦涩是形式性法治的对立面（值得注意的是，富勒的表述似乎允许一定程度的晦涩，只要受过训练的专业人士能理解——那么计算机科学家的协助是否被允许？还是说行会忠诚会排除这种可能？）。对机器学习系统（无论是否用于审判）的一个常见异议是，用富勒的话来说，对处于分类结果接收端的人而言，这些系统可能像是“晦涩难懂的杰作”（Pasquale, 2015）。自动化预测工具的奖励函数被编入代码，受其约束的人未必能轻易获取并审查这些代码；即便能获取代码，“普通公民”或“受过训练的律师”也难以理解其含义。与一些技术乐观主义者的观点不同（Fagan and Levmore, 2019），似乎不太可能有很多人能即时评估预测算法的质量。无论是面对Compas这类非机器学习工具，还是Midas这类机器学习工具，刑事被告或福利申领人都无法有效理解或质疑那些决定他们能否获得自由或生存资源的规则。（<span style="color:blue;">算法难以被相对人理解所引发的问题，与形式化法治要求的冲突。</span>）</p>

    <p>·对上述观点的深入分析与思辨：</p>

    <ol>
      <li><span style="color:blue;">难以理解不仅发生在ML中，人类主导的决策系统也存在这个问题。</span>正如人类主导的审判系统也会面临“难以理解”的异议，机器主导的系统同样存在这一缺陷（例如测速摄像头）。我们没有理由认为，新治理工具早期采用者面临的问题会在后续应用中持续存在。</li>
      <li><span style="color:blue;">ML也存在被解释的可能。</span>此外，计算机科学家已开发出一系列“可解释人工智能”（explainable AIs），通过向终端用户提供多种信息来阐明机器决策（Huq, forthcoming 2021）。例如，机器学习审判工具完全可以用通俗语言说明其奖励函数、决定结果的最关键参数，以及哪些行为或因素的改变可能导致不同结果（当然，这一过程可能会让规则更清晰，也可能使其更晦涩）。此外，算法语境下的透明度研究强调了一些在形式性法治研究中被淡化的考量因素。</li>
      <li><span style="color:blue;">解释的方式有很多种，不必然追求批评观点中ML难以做到的那种解释方式。</span>最重要的是，“解释”这一概念既不简单也非单一——相反，解释可分为因果解释、反事实解释、动机解释、物理解释等多种形式（Miller, 2019）。</li>
      <li><span style="color:blue;">解释本身的必要性——对“解释”要求的前提的质疑：披露不一定推动信任和有效反应。</span>“披露信息能产生信任并促进人类有效行动”这一假设也受到了挑战，理由是个体事前拥有的认知和社会资源不同，因此对解释的有效回应能力也存在差异（Ananny and Crawford, 2018）。但对于“仅能被‘受过训练的律师’理解和适用的法典”，也存在同样的问题。</li>
    </ol>

    <p><b>在可预测性和稳定性方面，算法存在优势</b></p>
    <p>（针对算法考量因素有限、个案裁量较小这一特点）</p>

    <p>不过，从形式性法治的视角来看，自动化决策在其他方面可能优于人类决策。形式性法治理论的一个核心主题是法律系统运作中的稳定性和可预测性价值。Raz强调，法律的可预测性（或其指导能力）是其法治理论的核心（Raz, 1979, p.218；另见Barber, 2018, pp.94-95）。Shapiro认为，法治的价值“完全来自社会规划产生的收益，且当法律结构最大化这些收益时，法治才能得到最佳实现”（2011, p.396）。如果法治被理解为对个人和社会规划的工具性支持，那么从人类决策转向机器学习决策有可能提高确定性——因为它消除了系统中一种特定的不确定性来源。一线官员行使个人判断时的随机性影响，很可能比自动化工具的运作更难预测。（<span style="color:blue;">相较于人类，算法在决策时限缩了考量因素，因而排除一些人为因素。</span>）这并非说从人类到机器的转变消除了所有形式的自由裁量权：哈特（H.L.A. Hart, 2013, p.661）所指出的“事实无知”和“目标不确定性”这两个自由裁量权的根源，无论选择何种决策工具，都可能持续存在。但通过集中决策过程，自动化至少内在地具有提高（而非降低）确定性的可能性。从“通过指导促进社会和个人规划”这一角度理解的法律，会因这种转变而得到强化，而非削弱。</p>

    <h4>【ML与程序性法治的互动】</h4>
    <p>（算法相较于人类过程，缺少互动与倾听，被认为不符合过程法治理念）</p>

    <p>机器学习工具对程序性法治的影响初看之下似乎更明确。沃尔德伦（Waldron）的程序性法治理论强调司法程序与维护尊严和能动性利益的关系。由人类主导的程序将个体视为“理性和智慧的拥有者”，尽管这在一定程度上会牺牲“法律的确定性”（Waldron, 2011, p.19）。在沃尔德伦看来，法治体系中的个体并非像富勒和拉兹理论中那样被动接受法律，而是通过论证和“详尽的解释性实践”积极质疑法律（同前引，第20页）。基于这种人类能动性观点，失去能够倾听并回应受法律约束者所提出的论证和事实的人类法官，将对程序性法治造成致命打击。机器工具那种冰冷（且字面意义上）的非人性冷漠，与程序性法治截然对立（O’Neil, 2016）。</p>

    <p>·回应与更深入分析</p>

    <p>然而，尽管沃尔德伦的论证很有说服力，但“尊严和能动性与人类主导的程序相关联”这一观点，并不像初看时那样显而易见或牢不可破。</p>

    <ol>
      <li><span style="color:blue;">现实中人们大多不具备实质性参与对抗程序、发表意见的能力。</span>首先需要注意的是，沃尔德伦所设想的受法律约束的个体，不仅是“理性和智慧的拥有者”，还具备特定的性情和气质——他们好争辩，且拥有智力和物质资源。正如富勒反对“晦涩难懂”的论证基于对一定背景资源水平的假设，沃尔德伦的理论也忽略了现实中面对法律的个体的普遍状况：他们经济拮据，被工作或家庭的日常琐事缠身，甚至可能因自身的女性、非顺性别者、跨性别者、少数种族或族裔、残障人士或非公民身份，在社会中频繁遭遇condescension、轻视、厌恶或怜悯等微妙而普遍的侮辱。这一系列身份的列举，凸显了沃尔德伦所设想的“好争辩且拥有权力的个体”实际上多么罕见。例如，在美国，刑事司法系统的运作规模庞大，实际上已达到工业化程度，这一点已得到广泛认可（Beckett, 2018）。据估计，美国每年处理约1320万起刑事轻罪案件（Stevenson and Mayson, 2018），其中很少有被传讯的被告符合沃尔德伦所描述的形象，情况恰恰相反。</li>

      <li><span style="color:blue;">增加过程中的参与、人际互动不一定能够体现对人的尊重。</span>

    <p><span style="color:blue;">（1）分情况的</span></p>

    <p>沃尔德伦的理论可能在动机和制度层面均不适用。他假设尊严和自主权通过对抗性程序得到提升，但对许多人而言，对抗性程序很可能是痛苦甚至污名化伤害的来源（Natapoff, 2015）。正如费利（Feely, 1978）所言，程序本身往往就是惩罚。例如，民事或刑事司法的管理型系统，仅通过给予听证这一行为，就可能对个体造成心理伤害。在申请福利或抗辩轻罪时，尽量减少人际互动，可能更能体现对自主权和尊严的尊重（Huq, 2020）。</p>

    <p><span style="color:blue;">（2）随时代、观念而变化</span></p>

    <p>尊严和能动性的最佳实现方式，还取决于对“公平和尊重对待”的背景预期。在20世纪末，很难想象其他可能性，但如今的社会技术环境已不再清晰。大众可能已愈发习惯与私人主体的自动化互动，包括医疗问题、人际关系（如在线约会论坛）和社交媒体互动等领域。推动自动化审判兴起的技术进步，也在催化人类预期和行为的更广泛变化。</p>
       </li>
    </ol>
    <p>在这种背景下，“如何构建与政府的互动以最佳促进尊严和能动性”这一问题，在很大程度上应被视为一个实证问题。至少，像沃尔德伦那样假设“机器取代人类判断必然导致尊严和自主权的丧失”，似乎并不明智。</p>

    <h4>【总结】</h4>
    <p>简而言之，机器学习驱动的审判与形式性、程序性法治概念之间的关系，并不像初看时那样简单直接，而是存在诸多经验性变数。对这些关系的简要思考，也揭示了法治中一个易被忽视的维度：其与背景社会经济条件的关联，以及法律体系治理选择的变化（包括是否使用机器学习工具的决策）对这些背景条件的改变方式。</p>

    <p>至少在形式性和程序性表述中，主流法治理论可能隐含着一个经验性预设——即法律面前人人享有大致的物质与智力平等。若缺乏这种平等，仅遵守形式性或程序性法治，可能会对受监管群体产生不均衡的影响：拥有物质和智力资源的人能充分利用法律的“指导”进行有益规划，而缺乏这些资源的人则难以从法治中获益。因此，对法治的坚持，其最终效果可能是加剧最初导致差异的权利不平等——即便如E.P. Thompson著名观点所言，法治让所有参与者的绝对处境都得到改善。据此，在某些（尽管非所有）情况下，法治可能成为社会不平等的推手。正如威尔莫特-史密斯（Wilmot-Smith, 2019, pp.90-91）所主张的，要抵消这种影响，法律体系可能需要为个体提供“等量的法律资源”，确保法律程序促进法律权益与负担的平等分配，包括“合法性”这一公共利益。与此相关但更抽象的观点是高德尔（Gowder, 2016）的主张：法治“必须能基于与所有人平等一致的理由，向所有人证明其正当性”。</p>

    <p>新技术让这一局面更加复杂。若形式性与程序性法治概念的互动本身就模糊不清，那么新型计算技术的最重要影响，或许在于它们对认知、社会和经济属性背景分配的作用——这些属性是法治成为可能且具有吸引力的前提。机器学习和人工智能工具并非分配中立的：它们倾向于利好那些能获取大量数据（如政府和大型商业机构已掌握的数据）及拥有从数据中提取推理所需技术资源的主体。无论是公共还是私营机构，若获得一种新方法，能以普通个体无法做到的方式利用现有资源，将改变个体与机构之间的社会经济权力平衡。随着管理和操控大量个体的新工具出现，权力可能会进一步集中到机构手中。此外，要免受这种新型权力的影响，需要隐私保护——即有权向他人隐瞒信息。但隐私分配本身也不均衡：在美国，国家机构常以种族和性别为由故意剥夺某些群体的隐私，例如，将非裔女性使用比例较高的某些福利与隐私剥夺挂钩（Bridges, 2017），其他地区也存在类似情况。因此，许多技术批评者合理担忧，即便新预测工具未能识别相关机制或弱势群体（Cuéllar and Huq, 2020），也可能扩大社会和物质不平等（Zuboff, 2019）。高德尔（Gowder, 2018, p.83）认为，某些技术创新“具有推动法治和司法可及性向平等方向发展的独特潜力”。但如果悲观者的观点成立，且威尔莫特-史密斯所倡导的“平等分配法律资源”仍更多是愿景而非现实，那么随着技术变革与背景下的财富和阶层等级制度相互作用并加剧后者，法治的实际质量将在实践中下降。</p>

    <h3>三、在后机器学习时代理论化阐述法治</h3>

    <p>技术的突然变革时刻，最终为我们提供了一个契机，去重新审视法治理论中那些基本却通常未被检视的预设。基于前文对沃尔德伦程序性法治理论的思考，本章将以“机器学习工具的出现是否会影响法治的概念界定方式”这一问题作结。更具体地说，通过反转分析视角可以发现：法治的规范性目标与其制度性必然结果之间的关联，可能比普遍认为的更为松散。尤其是“法院作为制度对法治至关重要、甚至不可或缺”这一假设，很可能需要重新考量。</p>

    <p>许多关于法治的开创性文献都明确或隐含地假定“人类管理的法院”处于核心地位。例如，拉兹（Raz）强调“司法独立必须得到保障”（Raz, 1979, pp.216-217）。正如加德纳（Gardner）恰当指出的那样（2012, pp.208-210），富勒的理论不仅与“无规则的法律体系”理念相悖，也与“无裁决的法律体系”（即缺乏官方对具体案件作出、旨在适用法律规则的权威裁决）的可能性相冲突。因此，富勒的理论要求一个“坚定独立、不回避裁决的司法机构”。最明显的是，沃尔德伦（2011）的程序性理论将法院和人类主导的审判过程置于核心位置。但正如塔克马（Taekama, 2020, p.5）所观察到的，像拉兹这样的理论并未证明“从法律的指导功能可推导出法院的必要性”。这一观点可进一步推广：形式性和程序性法治概念明确了法治的规范性目标（指导功能、尊严与能动性）。在过去某个时期，“这些规范性目标的制度性必然结果包括法院这类审判结构”或许是合理的假设，但无论过去是否如此，如今情况已不复存在。</p>

    <p>与法治相关的社会和人类福祉，可与那些已与之关联的特定制度形式剥离开来。技术进步催生了新的可能性——既可能背离这些价值，也可能以新的方式实现它们。无论如何，法治的规范性、概念性和制度性要素（理论家们通常将其打包整合为一个整体），其关联程度远低于普遍认知。</p>

    <p>本章的目的并非积极阐述“在全新技术环境下如何实现法治”——这将是一项庞大的任务。但至少，新型决策与预测工具的出现不应仅仅成为欢呼或绝望的理由，而应被视为一个契机，让我们重新思考那些此前或许过早打包整合到“法治概念”中的要素。</p>

  </article> 
