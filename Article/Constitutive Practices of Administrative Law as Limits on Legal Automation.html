<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<title>Constitutive Practices of Administrative Law as Limits on Legal Automation</title>
<style>
    body {font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; background: #fafafa; color: #222;}
    h1, h2, h3, h4 {color: #0f172a;}
    p {margin: 0.6em 0;}
   details {
   border: 1px solid #ddd;
   border-radius: 8px;
   padding: 1px 5px; 
   margin: 0px 0px;
   background: #f9f9f9;
   font-size: 15px; 
   }

  summary {
  display: block;       
  padding: 6px 6px;
  font-weight: bold;
  font-size: 16px; 
  cursor: pointer;
  color: Green;
  background: #fff;
  border-bottom: 1px solid #ddd;
}
  .container {
  max-width: 1000px; /* 原来是 700px */
  margin: 3rem auto;
  padding: 0 1rem;
}

    .toc {background:#fff; padding:1em; border:1px solid #ddd; border-radius:8px; margin-bottom:1.5em;}
    .toc ul {list-style:none; padding-left:0;}
    .toc li {margin:0.3em 0;}
    .toc a {text-decoration:none; color:#0ea5a4;}
    .toc a:hover {text-decoration:underline;}
    </style>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>Constitutive Practices of Administrative Law as Limits on Legal Automation</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <main class="container">
    <h1>Constitutive Practices of Administrative Law as Limits on Legal Automation</h1>
    <p>作者：Frank Pasquale</p>
    <p>引用：Frank Pasquale, <i>Constitutive Practices of Administrative Law as Limits on Legal Automation</i>, 1 Journal of Cross-disciplinary Research in Computational Law 1 (2025).</p>
         <hr />

<h2>🗂️大纲</h2>
     <hr />
<h2>🔭摘要</h2>
    <p>当前，自动化法律系统的吸引力建立在三大支柱之上：速度、规模与偏好满足。然而，对于法律系统的诸多方面，人们普遍认为将其转化为计算过程并不适宜。这种对过早或不明智的自动化的担忧体现在多个层面：自然语言相较于计算机语言所具备的灵活性至关重要；对法治的“法律过程”解读，关键在于是否存在人工审查、上诉机制及对话式互动的可能。</p>
    <p>对于技术拥护者而言，针对这些关于法律自动化局限性的具体论述，一种可能的反驳是将批评者对现有法律流程的维护定性为“僵化论”（Ossification）或“自然化论”（Naturalization）。“僵化论”指将本属偶然形成的实践病态地固化为永恒不变的模式；“自然化论”则是将人为构建的流程当作自然法则般对待，其前提是一种错误的假设——认为这些流程具有持久的存续性或价值。</p>
    <p>法律系统兼具可塑性与构成性实践的普遍性，这促使我们从两个层面展开思考：a）自由法律秩序的哪些方面至关重要；b）对于这些至关重要的方面，当其被部分或完全自动化时，会丧失哪些核心内涵。在自由法律秩序这类人类活动领域中，某些行为模式仅为实现特定目的的工具，而另一些则具有本质性、构成性的意义——一旦这类实践消失，该活动便不应再被视为自由法律秩序的一部分。</p>
    <p>那些仅为案件合法解决提供辅助、具有工具性的行政流程，非常适合自动化。而其他由人执行、为了人而存在的实践，则具有本质性与内在重要性，理应拒绝被转化为机器可读取的代码。区分法律中“辅助性与构成性”、“工具性与内在重要性”的不同方面，既应成为法律自动化的边界，也应作为其发展的指引。</p>
      <hr>

<h2>📝主要内容</h2>
    <h3>介绍</h3>
    <p><b>【背景与问题】</b></p>
    <p>在公法与私法中，法律自动化都在快速发展。合同的自动缔约、现有法律协议监控、纠纷解决等。部分法律咨询、法律服务也融入了算法，法律类聊天机器人旨在解决司法救助的可及性问题，为用户提供一系列简单案件所需的法律文书。</p>
    <p>这些倡议很多都是值得称赞的，提升了司法的可及性以及商业效率。但是，过度使用AI就会出现问题，而现在的趋势恰恰正倾向于过度使用AI。过度的自动化可能使它们本来试图解决的问题恶化。</p>
    <p><b>【认为法律自动化不可行的观点】</b></p>
    <p>这种对自动化操之过急或不够明智的担忧，已通过不同角度被阐释。</p>
    <p>立法：自然语言所具有的“灵活性”flexibility是关键性的，这区别于计算机语言。立法者对自身能否预见法律可能适用的所有潜在情形缺乏信心，因此他们更可能认为，相较于计算机代码，语言的灵活性是表达法律所蕴含的承诺的最佳方式。</p>
    <p>个案裁判：决策者的身份与责任也同样关键。Kiel Brennan-Marquez与Stephen Henderson认为，人类的同理心对司法系统施加惩罚的正当性是关键的。法治的“法律程序”也依赖于可行的人工复核、申诉机制以及对话式的互动机制。这些关于法律自动化局限性的具体论述，对学者与政策制定者都产生了影响。</p>
    <p><b>【支持自动化、反对上述观点的理由】</b></p>
    <p>支持自动化的观点拒绝接受上述批评者将现有法律程序描述为僵化固化或天然合理的观点。</p>
    <p><b>僵化固化ossification</b>：把一些仅仅是偶然的、依情况而定、可能存在负面效用的做法/实践固定为永久、固定不变的要求。</p>
    <p><b>认为天然合理的naturalisation</b>：将人为构建的程序（如本文所讨论的法律实践）视作类似自然法则的事物，进而错误地认定这些程序会持久存在或具有固有价值。</p>
    <p>在过去的几个世纪甚至近几十年里，合同的写法、对合同争议的裁判都发生了巨大的演进变化。公共行政领域一直是新自由主义管理主义旨在“重振”的对象。反驳“批评人工智能者”的人认为，我们不应陷入怀旧情绪，不能仅仅因为传统就固守旧有的纠纷解决或规划模式。这一反对观点颇具说服力，尤其是如今评论人士乃至法律改革委员会都在探讨如何让法律更易于实现自动化。</p>
    <p><b>【本文的内容——理由：法律的过程非常重要】</b></p>
    <p>为了回应naturalisation和ossification挑战，本文致力于提供一个更加一般化的论述，关于法律为什么是或应当比驾驶、制造生产标准化商品、维持房间的温度等更难以自动化。</p>
    
    <details>
      <summary>那些日常性的、能够被轻易自动化的活动与法律适用存在两方面的区别：<br>
        <details>
          <summary>
          （1）过程是否有独立的价值；
          </summary>
          <p>其他活动都是结果导向的，为了实现结果，实现的方式并不重要，因此技术可以轻易地改变/革新实现方式。但法律的实现方式与结果/目标有各自独立的价值。</p>
        </details>
        <details><summary>
           （2）是否有已知的、明确的最优化结果。
          </summary>
          <p>其他能够自动化的活动都存在一个已知的、明确的最优化结果。因此可以以确定的目标为导向来优化实现方法。但法律适用缺少一个明确可测量的最优结果，只能通过特定的过程来证明结果的正当性。</p>
          <p>当法律适用不存在一个有着特定共识的最优结果时，过程就变得重要。<br>（<span style="color:orange;">这可以是一个进一步细化分析的标准，对于哪些最有结果是固定的案件类型，如简易诉讼，是否就可以用自动化？</span>）</p>
        </details>
          <em>&nbsp;&nbsp;&nbsp;&nbsp;（2）可能是（1）的根本原因。</em>
      </summary>
    <p>这些更为日常的活动中，没有哪一项的构成性实践（过程）是与其旨在实现的目标相互割裂、彼此独立的。技术完全可能从根本上改变当下完成这些活动的方式。</p>
    <p>具有更强规范性意义的社会现象（例如由国家行政机关进行的法律纠纷解决），与机械重复、可程序化的行为，或是从一开始就有明确目标的服务（如物流、驾驶）截然不同。人们无法在地图上标注“正义”的位置，也无法为实现正义设定一套固定的算法步骤。尽管行政机关的（纠纷解决）流程可能会随时间演变，但它们不能失去构成其本质的核心实践——否则这些流程就会完全变成另一回事。</p>
    <p>部分原因在于，在法律领域的诸多场景中，待实现的目标或结果从一开始就是未知的。判断某一程序的结果是否具有合法性（若该结果处于不违背实体正义原则的合理范围内），最主要的依据便是确认这一结果是通过正当程序产生的。</p>
    <p>对结构、过程与结果的评估恰好能说明这一点，尤其当我们将法律行业与其他行业对比时，差异更为明显。以医疗行业为例，评估一家外科诊疗中心时，可能会考察其设施条件（结构层面）、医护人员的操作（流程层面），或是患者的康复状况（结果层面）。社会对“健康”的定义存在广泛共识，因此只要能整体改善健康水平，医疗行业的结构与流程就可能随时间发生显著变革。</p>
    <p>但法律行业则不同——当它处理的案件在作出裁决前（如存在争议的诉讼）或达成协议前（如合同谈判）没有明确“正确”结果时，情况便大相径庭。</p>
    <p>事实上，针对特定金融法规或环境法规的适用产生争议，本身就意味着社会对该争议的恰当解决方案存在一定程度的分歧。因此，在法律领域，核心在于“探寻真相”与“适用规范”的过程，而非法律被预设要达成的某一特定结果。</p>
    <p>我们不清楚一个案件中的哪方当事人应当胜诉，否则这个案件就会在诉讼早起就解决了。或许基本的方法例如“审结期限”可能可以作为一种评判法律系统质量的粗略指标，但这个指标很粗略：如果法院用扔硬币的方式来解决所有的案件，那么审结期限是最短的，但这与正义毫无关系。识别某个程序是合法的，就不仅仅要看他所旨在达到的目标，还要看程序本身内在的东西：构成性实践。</p>
    </details>
    
    <p>我对这些构成性实践的讨论将聚焦美国法，特别是法律自动化领域诸多学者所熟知的一个争议领域：行政听证。</p>
    <p>行政法专家已捕捉到借助自然语言处理（NLP）、人工智能（AI）和机器学习（ML）实现自动化的前景。2019年发布的一份里程碑式报告中指出了在行政国家多个维度实现自动化的大量机遇。</p>
    <p>尽管他们强调并认可了诸多有价值的举措，但人工智能在行政听证中仍将面临长期挑战。因此，法律自动化必须审慎推进，甚至完全不应推进，以防止忽略那些由直接的人类参与所构成的核心实践。（<span style="color:orange;">哪些环节/价值必须人类参与才能实现？</span>）</p>
    <p>下文题为“行政听证中正当程序的构成性实践”的部分，将探讨美国行政听证中正当程序的经典维度如何被视为法律的构成性实践——因此，这些维度尤其难以通过自动化手段被忽略、简化，甚至难以满足实现自动化所需的那种全面标准化要求。本文结尾将反思：认识到某些至关重要的人类实践无法被算法还原这一特性，可能会如何丰富未来关于法律科技的讨论。</p>
 <hr style="border: none; border-top: 1px dashed #000;">
   
    <h3>正文：行政听证中正当程序的构成性实践</h3>
    <p>法律体系所兼具的可塑性与体系内构成性实践的持续性，引发了两个层面的思考：a）在法律程序的某一环节中，人类的参与及相关技能何时对该环节的合法性至关重要；b）当这种人类的参与及技能被计算技术部分或完全取代时，会失去什么。</p>
   <p>本部分将围绕Friendly factors展开一系列思考，对上述两个层面分别进行探讨。</p>
      <div style="border: 2px solid #333; background: #f0f8ff; padding: 16px;">Friendly factors指美国行政程序中的11个方面，法官Henry Friendly认为这些方面在正当程序的认定中至关重要，列举如下：
    <em><b>
    <p>1. 无偏私的裁判庭</p>
    <p>2. 告知拟采取的行动及其所依据的理由</p>
    <p>3. 陈述反对该行动的理由的机会</p>
    <p>4. 传召证人的权利</p>
    <p>5. 知悉对方证据的权利</p>
    <p>6. 裁决仅基于所提交证据作出的权利</p>
    <p>7. 获得法律代理（咨询帮助）的权利</p>
    <p>8. 制作案卷记录</p>
    <p>9. 可获得对裁决理由的说明</p>
    <p>10. 允许公众出席/参与</p>
    <p>11. 司法审查</p>
    </b></em>
   </div>
   <p>可将这些因素归为四类：a）听证方面（第 1、10 项）、b）被裁决者的权利（第 2、3、4、5、7 项）、c）裁决者的义务（第 6、8、9 项）、d）审查权（第 11 项）。</p>
   <p>通过探讨行政听证上述各维度实现自动化的可行性，我希望阐明：<b>哪些要素应被视为自由法律秩序的构成性要素，而哪些要素仅因特定时空条件而存在</b>（具有偶然性、依情况而定的）。</p>
    <p>在自由法律秩序这类人类活动领域中，<b>某些行为模式仅为实现目标的工具，而另一些则是核心且具有构成性的要素</b>——当这类构成性实践消失时，该活动便不应再被视为自由法律秩序的一部分。（<span style="color:green;">在法治的要求中，又有工具性的与本体性之分<span>）</p>
      <p><b>【本文的分析方法、内容（所依据的论文）可能面临一些质疑】</b></p>
    <p>时间上：1970年代的论文，后续许多类似案件并非完整地要求11项要素；</p>
    <p>地域/法域：只考察了美国的普通法系，没有考察其他国家和大陆法系；</p>
    <p>法律层级：只考察了联邦层级；</p>
      <p>【<b>对质疑的回应</b>】</p>
    <p>Friendly的列举、分析仍然是美国行政法和正当程序的检验标准touchstone。</p>
    <p>这篇论文在这里主要作为一个面向对保护当事人感兴趣的计算法律的实践者的有用的、整理好的（思考）框架，而非是一个对自由法律秩序一般性的权威论述。</p>
    <p>后续的研究可能会探讨：<span style="color:orange;">在Friendly factor的基础上，哪些增删调整可能成为美国以外地区的典型特征，或者甚至成为美国本土法理学发展趋势的典型特征。</span></p>
    <hr style="border: none; border-top: 1px dashed #000;">
 
<h3>第一部分 听证方面：无偏私的裁判和公众的出席/参与</h3>
      <p>【<b>无偏私的裁判</b>】</p>
    <p>有一些类型的偏见可能会损害当事人在决策者面前的公平机会。</p>
    <p>1、关联偏见associational bias：当裁决者可能与案件一方存在商业或社会联系时，就存在关联偏见的风险。然而在实践中，本应监管此类情况的行政机关及法院，均不愿对这类偏见展开深入审查。</p>
    <p>大法官Scalia曾因参与审理一桩涉及时任副总统Dick Cheney的案件（二人曾一同猎鸭）而受到批评，他对此表示：“若一项规则要求本院大法官在案件涉及友人的公务行为时回避，那将完全无法开展工作。”因此，实用主义占据了主导地位。</p>
      <p>鉴于最高法院大法官乃至许多下级法院法官的社会地位，他们“交往的圈子”很可能比普通美国公民更富有、也更保守。<b>但这类偏见在很大程度上被认为过于分散，不足以针对特定法官采取行动。</b></p>
    <p>2、substantive bias：裁决者在案件呈送给他们时就已经做出了裁决。</p>
    <p>Rule：（禁止）裁决者在案件呈送给他们时就已经做出了裁决prejudgment。</p>
    <p>在传统领域的表现：</p>
    <p>法院：法官曾谴责过某个公司的行为，或许不会被允许在法庭裁判这类行为。</p>
    <p>行政机关：在行政机关的领导层，这种情况会变得更加复杂。虽然：整个机构例如FTC，拥有准行政权、准立法权和准司法权。不去理清该机构的首要任务是无法领导这种规模的机构的。这种确定行政机关首要任务的裁决权的行使也会存在偏见。FTC反垄断强制覆盖的产业中，只有一个或几个公司可能会占据市场主导地位。例如，当谈论美国的社交媒体产业时，或多或少会谈论到Meta及其收购项目。所以仅仅是谈论或分析特定公司的行为不能够被认为是违反了不得预先裁判的要求，除非具有更多的证据和上下文场景。当行使准司法权时，行政机构的决策者会被认为没有资格，如果他/她在听证前就对某个案件的事实或法律适用做出了宣判。</p>
    <p>Analysis：在算法问题上</p>
    <p>对于自动化的法律裁判，该项要求引起了令人困惑的考量，即何为预先裁判prejudgment？一个自动化的系统可能只能以二分或粗略的方式识别一种事实模式。当算法分类器没有很好地适配案件时。这是否是一种违法的预先裁判？如果是，是否需要某种人类干预来使其更更精细的分类？随着机器裁判在行政决策中成为可能，这些重要的问题将需要被更直接地解决。（<span style="color:green;">算法这些新事物使得原本的规则出现了迷惑，要解决这些问题，还需要对传统规则有更深入/更新的理解。例如这里就需要对prejudgment重新理解。</span>）</p>
    <p>无偏私的裁判庭的要求很少被提起，但仍然是美国法上重要的部分。它不能够被完全省略掉，即便它只在很有限的方式被适用。自动化的过程能够满足这个标准吗？对此的观点是分化的：一方的观点认为，存在一种形式化的法律概念，这能够看到根植于人类情感或意识形态的偏见。在这种逻辑下，在裁决时，机器能够更少地考虑无关的因素。</p>
    <p>另一种观点认为，考虑到许多法律要求都是开放性的，关于哪些因素是无关的这个问题本身就需要一种判断，该判断也可能存在偏见。</p>
    <p>所以，对那些确定性较强、结果封闭的法律程序的自动化可能不会涉及中立裁判的要求（在其中所有相关的因素都已经被作为一种法律规则提前确定，并且有达成共识的将其转化为算法的方法），对那些更具有开放性的法律程序的自动化将需要更多的关注。（<span style="color:green;">以确定性标准来区分法律程序是否可以被自动化</span>）</p>
    <p>【对算法进行审查，防止算法中本身包含着偏见】</p>
    <p>在算法审计中对算法的详细查看是必要的，以确保自动化决策本身不会构成或恶化一个偏私的裁判庭。这种审计需要确保算法的输入和输出均是合法的。</p>
    <p>在输出端，一个持续对特定群体的人产生不利决策、这种几率高于平均水平的，自动化决策程序应当被标记为需要进一步的调查，并且或许值得停止仅以该算法为裁判依据。（<span style="color:green;">通过测试、结果的概率来测试输出端</span>）</p>
      <p>在输入端，开发自动化决策系统的人应当特别谨慎，确保所使用的数据是具有代表性、无偏见的。（<span style="color:green;">前端，开发者来确保数据、算法</span>）</p>
    <p>除此之外，自动化决策系统所设置的主要目标不应当是单一片面的，例如“削减福利”。美国和澳大利亚的实践表明，这种单一目标会导致算法倾向于对事实和法律做出错误或牵强的解释，以实现设定的政治目标。</p>
      <p>【<b>公众出席/参与</b>】</p>
    <p>公众出席或参与的机会也应当是行政听证中的规范之一。在这方面，技术的发展与透明度的目标保持了一致，借助互联网等通讯技术，公众的出席变得更加容易了。</p>
    <p>但出于隐私（包括当事人和决策者）的考量，行政官员可能决定将某些敏感的程序保留在公共视野之外。他们也可控限制访问特定的程序或甚至保护关于某些程序的数据。这些做法表明合理的公众参与以及对裁决程序的理解是这项要求的核心。（<span style="color:green;">公众参与要适度，参与的方式、参与的深度控制在合理范围内，并非绝对性要求。</span>）</p>
    <p>这里在介绍法国司法公开的实践时提到了与美国法律文化的差异：美国是一种个人为中心的模式，表现为法院的裁判主要围绕个案展开论述，法官的意见也会以个人的名义在判决中体现（如不同意见）。法国则是一种机构为中心的模式，表现为法院的裁判是一种非个人化的表述，法官的意见并不会以个人名义出现，也没有人知道三人法官合议中个人的意见。</p>
    <p>因此，在法国，外界可以针对某个法院的裁判做出分析和预测，但不能针对某个特定法官进行分析和预测。</p>
      <p><b><em>对透明度的理解</em></b>：</p>
    <p>David Pozen认为，透明度本身并不是重点，而是一种为了实现某种社会善的途径。因此它不是一种超验的规范性存在，而是一种行政管理工具，就像其他公共管理工具一样，有着可被质疑的道德、政治以及分配方面的后果。（<span style="color:green;">（面向公众的）透明度并非自由法治的必要组成部分，只是一种手段。</span>）</p>
    <p>这是一种很受欢迎的提示，提醒我们在大数据时代，并非所有技术能够实现的透明度都应该实现。</p>
      <p>公开和保密之间的平衡，透明度与隐私之间的平衡需要实践智慧，在个案中根据事实来个性化应用法律。<b>这种合理的平衡和个性化裁判需要人类的判断。</b></p>
    <p style="color:green;">（技术有利于实现向公众的透明/公众的参与，但透明度并非绝对，实现何种透明需要在个案中裁量，这种裁量需要人类的判断。）</p>
<hr style="border: none; border-top: 1px dashed #000;">
    
 <h3>第二部分 被裁判的人的权利：通知，提供理由、传召证人、知晓反对证据的机会</h3>
  <p>【<b>告知</b>】</p>
     <p>告知是自由法律秩序中的构成性实践。</p>
     <p>每个人都应当有权利了解针对他们的不利法律行动，及其依据。具体的实现方式可以是多样的：（1）有时通知的要求需要通过真实的<b>一对一</b>通知来实现。例如，当事人签收了文件表明他收到了通知，或通过视频记录可证实已向其告知拟采取的行动并提供了相关文件。（2）在其他情况下，<b>推定告知可能已足够</b>——例如，受影响者可被视为已知悉某出版物上的公告，或（在规则制定程序中）已知悉《联邦公报》（Federal Register）上发布的内容。（<b>一对多的告知</b>）</p>
    <p>威权主义者可能更青睐秘密法庭（star chamber），因为其效率高得多 —— 毕竟告知义务不仅给予受影响方为自身辩护的机会，也让他们有机会逃脱。</p>
    <p>尽管如此，自由法律秩序要求被指控者拥有一定的机会陈述为何不应采取某项行动——这是法律的又一项构成性实践。</p>
    <p>当然，也存在“微量或琐碎事项”的例外情形：常理表明，必定存在某种底线，低于该底线时，则无需进行任何形式的听证。……在这类案件中，举行证据听证的成本，与更准确裁决的可能性或价值相比，已严重失衡——因此，最终应依赖项目管理者基于充分信息且秉持诚信的判断。（<span style="color:green;">告知的需要是一种成本-收益分析，当告知所带来的收益小于告知所需的成本时，告知是不必要的。</span>）</p>
    <p>Henry Friendly法官对额外程序价值的隐含量化，预示了美国最高法院在Mathews v. Eldridge中的重要裁决——该裁决要求，诉讼当事人需对其请求的额外程序可能带来的准确性提升程度作出大致估算。交通处罚就属于这类领域，其涉及的利害关系足够小，而更长时间程序所带来的益处又微乎其微，因此在处以处罚前无需进行告知（进而也无需提供陈述理由的机会）。</p>
  <p>【<b>给出理由/证据</b>】</p>
    <p>Rule：</p>
    <p>知道针对自己的（不利）证据意义上的透明度也对自由法律秩序至关重要。被剥夺获取此类证据的权利，会损害对“讲述自己视角的故事”的保障。然而，在证人的作用受到合理质疑的情况下，传唤证人的权利可能会受到限制。换言之，诉讼当事人陈述案件的方式或许可以多样，但必须知晓对方当事人指控或主张的依据。</p>
    <p>Analysis：算法</p>
    <p>很难想象，在使用“黑箱”式、无法解释的AI时，诉讼当事人如何能“知晓”对方的主张依据——这一缺陷可能再次严重限制法律自动化的范围。至少，用于“指控”嫌疑人的人工智能需要披露其使用的数据，或许还需披露所采用的算法。法学教授Chad Squitieri已指出该问题涉及的宪法层面考量：</p>
    <p>当大数据、算法的分析结果成为指控个人的理由/依据时，这些结果就如鉴定报告、检测证据一样，算法背后的人类开发者应当受到质询。宪法第六修正案在刑事案件中对质的要求。</p>
    <p>Squitieri的研究尤为具有启发性，因为它强调了人工智能背后不可替代的人类角色。在生成相关工具的每个环节中，若人工智能的人类创造者身份明确，且需对其在各环节作出的选择负责，那么该人工智能的合法性便会相应增强。（反之，如果AI背后的开发者对AI一无所知，那么AI的可信度、合法性就会下降）</p>
  <hr style="border: none; border-top: 1px dashed #000;">
  
<h3>第三部分 裁决者的义务：仅依据所提交的证据作出裁决，并说明裁决理由</h3>
    <p>【<b>仅依据所提交的证据作出裁决</b>】</p>
    <p>仅依据所提交的证据作出裁决时Friendly因素中最争议的要素之一。每一个裁决者都会把特定的经验和理解带入到案件中。这些都至少会影响裁决者对证据的接受并且在某些情况下可能成为决定性因素。</p>
    <p>Rule：</p>
    <p>因此，或许可将这一义务更好地重新表述为：不得搜寻或接收诉讼当事人未提交、亦未知悉的与特定案件相关的证据——并确保所有可向诉讼当事人明确告知的因素，均实际予以明确告知。换言之，制作案卷记录与说明裁决理由的要求，已包含了“仅依据所提交的证据作出裁决”这一要求。</p>
    <p>聚焦于案卷记录的制作与裁决理由的说明：这对于正式行政听证中的最终裁决至关重要。但该原则亦有其局限性：并非所有证据裁决都需说明理由，也并非所有程序决策都会接受审查。</p>
    <p>尽管如此，为保留制作此类记录与作出相关解释的可能性，受听证结果影响的当事人必须能够就裁决依据向裁决者提出质询，能够对留存的任何案卷记录申请审查（此为下一部分的探讨主题），并能够基于案卷记录的不充分或存在偏见，请求撤销相关行政行为。</p>
    <p>Analysis：</p>
    <p>对许多政府而言，将裁决者的这些关键义务（制作案卷记录与说明裁决理由）进行自动化的前景颇具吸引力。美国关于正当程序的判例法中，充斥着严重（即便具有误导性）的论断，称分配给程序保护的资金很可能会被从受助者手中克扣。[34] 社会对官僚主义普遍存在不耐烦与不满情绪。但与诸多其他拟议的政府改革一样，此事的关键在于细节。</p>
    <p>【<b>对裁决理由进行说明</b>】</p>
    <p>对法律裁决进行解释的要求，是此类法律自动化推广过程中的一个关键障碍。尽管 OpenAI 已声明，其公开可用版本的 ChatGPT 不提供法律建议，但大型语言模型（LLMs）的推广者或许很快就会宣称，此类技术能够撰写为裁决提供依据的法律意见。事实上，已有文献记载法官等法律从业者使用 ChatGPT 的情况。</p>
    <p style="color:green;">（法律中使用AI的几项规则：有意识地用、可以用，但要标明、要对AI内容进行核查和负责）</p>
    <p>由于担忧该技术可能被滥用，Juan David Gutiérrez教授提出建议：</p>
    <p>（i）使用者必须了解技术的工作原理，承认其局限性与风险，并确保该工具适用于所需完成的任务（知情使用）；</p>
    <p>（ii）使用者在（法律）程序中需透明化技术的使用情况（透明使用）；</p>
    <p>（iii）使用者需明确区分司法裁决或法律文件中哪些部分是人工智能生成的文本（伦理使用）；</p>
    <p>（iv）使用者需依据可靠来源，对从人工智能系统中获取的信息进行严格核查，并明确告知此类核查情况（负责任使用）。”</p>
    <p>这些均是在司法语境下采用自动文本生成技术的关键条件 —— 尤其考虑到研究科学家萨沙・卢乔尼（Sasha Luccioni）与安娜・罗杰斯（Anna Rogers）教授已指出当前大型语言模型评估方法存在缺陷。[37] 鉴于许多大型语言模型的基础层面（例如用于训练模型的文本语料库、基于人类反馈的强化学习（RLHF）方法等）均不透明，古铁雷斯提出的第一个条件在短期内也几乎无法实现。</p>
    <p>其他的担忧：</p>
    <p>即便古铁雷斯提出的四项条件全部得到满足，对于借助大语言模型（LLMs）“下一个词预测” 的核心特性来生成裁决理由，人们仍存在严重担忧。</p>
    <p>尽管“数据的惊人有效性”或许能让大语言模型通过Eugene Volokh所说的“约翰・亨利测试”（即生成的文书、口头提问与回应，即便经验丰富的法律专业人士也无法将其与其他资深同行的成果区分开来），但相关人工智能实际上既无法理解自身行为的含义，也无法产生相应的情感共鸣。</p>
    <p>对于Kiel Brennan-Marquez和Stephen Henderson等评论者而言，这种共情能力是国家权力获得合法适用的必要条件。</p>
    <p>这也是人工智能研究起步阶段，麻省理工学院（MIT）计算机科学教授约瑟夫・魏泽鲍姆认为司法程序自动化如此令人不安的原因之一：</p>
    <p>计算机无论能展现出何种智能，无论这种智能是如何获得的，它必定始终与人类所有真切的关切完全格格不入——这一事实难道不是再明显不过了吗？</p>
    <p>“法官所知晓的、而我们无法告知计算机的东西是什么？”——提出这样的问题本身就是一种骇人听闻的亵渎。即便只是为了揭露其病态而不得不将这个问题付诸文字，也已然是我们这个时代疯狂的标志…… 最基本的洞见由此浮现：既然我们目前尚无任何方法能让计算机拥有智慧，那么我们现在就不应该赋予计算机需要智慧才能完成的任务。</p>
    <p>每当使用“黑箱人工智能”时，这种意义理解上的问题便会加剧。“黑箱”包含两种含义：一是数据的黑箱，即个体无法获取其训练、决策所使用的数据；二是逻辑算法的黑箱，即算法过于复杂或与人类思维不同，导致无法拆解为一系列个体可理解的规则。人工智能的透明度越低，就越难以满足公开记录的要求。</p>
    <hr style="border: none; border-top: 1px dashed #000;">
    
<h3>第四部分 听证后的权利：就指称的事实与法律错误申请审查</h3>
    <p>Rule：在自由法律秩序中，允许对裁决进行复核review是一项坚定的原则，其目的是确保事实或法律错误不会对结果产生决定性影响。</p>
    <p>理论上，自动化法律系统能够提供此类审查。</p>
    <p>事实上，美国社会保障局（Social Security Administration）已出现针对极特定错误（例如，识别福利的不当拒发情况）开展有限自动化审查的动向。</p>
    <p>然而，任何熟悉上诉审理过程中所涉及的、具有语义复杂性的裁判判断（即判断是否受理上诉以及如何解决上诉）的人，都应对“机器人法官”完成此项任务的能力持怀疑态度。</p>
    <p>此处缺乏足够的数据支撑合格的自动化审查，尤其是考虑到许多上诉案件都属于“首例案件”（case of first impression）。</p>
    <p>此外，即便我们能假定存在一个调校极为精良的大语言模型，它生成的上诉判决在一段时间内能够获得法官的认可（如前文提及的Volokh“约翰・亨利”测试中所假设的那样），但其运营者该如何证明其观点的持续有效性呢？随着时间推移，应添加哪些类型的数据来“更新”算法？由于无法提前知晓所有待决案件的范围（这本身就无法预测），回答此类问题的难度之大，警示我们不宜将上诉审查职能自动化——而这一点更有理由（a fortiori）适用于多数关于“法律奇点”或普遍个性化法律的设想。</p>
      <hr style="border: none; border-top: 1px dashed #000;">
  
<h3>结论</h3>
        <p><span style="color:green;">上升为哲学层面的讨论，对现实世界形式化的失败</span>（从哲学上理解法律自动化，是一种把人类现实活动符号、形式化的过程。）</p></p>
    <p>（<span style="color:orange;">但人工智能是否是在形式化？不是试图用代码来模拟现实世界，而是从数据中学习模拟现实世界。不过也可能是一种形式化，因为AI只能从数据中学习，而从现实世界到数据的过程就是一种形式化。数据只能反映、记录现实世界中的一部分。</span>）</p>
    <p>尽管计算智能增强（Intelligence Augmentation, IA）有望为法律体系带来诸多改进，但有一些对自动化的规范性限制（normative limits），尤其是对于当前被大肆宣传的生成式人工智能。</p>
    <details>
      <summary>本文最重要的启示在于，应避免通过过早自动化来消解自由法律秩序的构成性实践。将复杂的社会过程（如法律实践）简化为可计算的形式化规则时，可能会丢失其中蕴含的语境、价值和隐性知识，即“遗忘”了实践的社会属性。</summary>
      <p> 在Agre将形式化描述为“一种高度结构化的社会遗忘形式”的经典论述中，下文的“行动”一词可替换为法律领域的诸多构成性实践：</p>
    <p>“对于一项旨在尊重日常用法的哲学研究而言，“行动” 这类词汇可能构成真正的挑战；但将 “行动” 归入形式语言理论的做法，会把这个词简化为一种远为初级的形式：即由一套离散、有限的 “表达元素” 或 “基本单元” 组合而成的可能 “行动” 的集合。一旦 “行动” 一词被纳入人工智能的专业术语体系，其原本的语义分支便不再是人工智能研究可利用的潜在资源。围绕形式化的意识形态，并未赋予这些被舍弃的内容任何内在价值。结果是，形式化沦为一种高度结构化的社会遗忘形式 —— 遗忘的不仅是词汇的语义，还有它们的历史属性。这也解释了为何人工智能核心思想的历史渊源与学术发展，在该领域从业者中鲜少引起关注。”</p>
    </details>
        
    <p>回想一下，在本文开头，我曾提及对人际互动构成性实践的 “自然化” 或 “僵化” 批判 —— 这种批判将这些实践仅仅视为传统而不予理会。从这种未来主义视角来看，（当下及未来的）法律工程师的角色范围极为宽泛，甚至带有全面支配性：所有实践都必须依据其结果接受审视，若存在能更高效达成这些结果的方法，就应当开发并采用这些方法。</p>
    <p>Agre的睿智之处在于，他强调了在所谓 “数据驱动” 的自动化流程中被遗漏的所有信息（包括决策瞬间对人的质性印象）。尤其在上述复杂的法律语境中，“数据驱动”一词应替换为“由特定数据选集驱动——这些数据仅代表某个特定时空下的情况，且始终需要更新”。[43] 正如Thea Snow所言：“我们应当不断自问：缺失了哪些数据？哪些人的声音被忽略了？我们正在做出哪些未经检验的假设？这些假设又如何遮蔽了其他真相？[……] 我们在世界中的立场与权力，如何影响我们 [……] 利用所收集的信息做出决策？”（隐性知识、难以被数据化，数据不够全面，数据不够新，重要的直观感受等）</p>
    <p>Agre曾花费数年时间，试图构建能够完成机器人在物理世界中行进所必需的各类搜索与导航任务的系统，以实现规划制定与任务执行。他深入钻研了人工智能领域多种研究方案的发展史 —— 从符号分析、联结主义，到如今已无处不在且成果斐然的神经网络早期原型。然而，在多年致力于实现人工智能核心目标之一后，他最终成为了该领域最尖锐的批评者之一。近期一篇致敬其成果的文章曾提及：</p>
    <p>“从某种意义上说，阿格雷开始反抗自己的职业。他主动接触人工智能的批评者，研习哲学及其他学科。他曾写道，起初这些文本对他而言‘晦涩难懂’，因为他早已习惯像剖析数学或计算机科学技术论文那样，拆解自己读到的所有内容。‘我终于意识到，不应再将这些陌生的学科语言转化为技术图式，而应直接从其自身逻辑出发去理解它们。’”[45]</p>
    <p>法律的构成性实践依赖于这些 “陌生的学科语言”，且抗拒标准化。阿格雷对人工智能的不满，源于他试图将人类经验的某些层面转化为形式化术语时遭遇的挫败。这种挫败感在公法自动化（通过公共行政领域的人工智能）与私法自动化（通过 “智能合约” 及类似技术）的批评者中反复出现：审判过程的某些环节，甚至许多商业交易的细节，根本无法简化为机器可读取的代码。当这些层面同时也是自由法律秩序的构成性实践时，不应仅仅为了使法律实践更易于自动化处理，就将其摒弃。</p>
 
</body>
</html>
