<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<title>The Scored Society</title>
<style>
    body {font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; background: #fafafa; color: #222;}
    h1, h2, h3, h4 {color: #0f172a;}
    p {margin: 0.6em 0;}
   details {
   border: 1px solid #ddd;
   border-radius: 8px;
   padding: 1px 5px; 
   margin: 0px 0px;
   background: #f9f9f9;
   font-size: 15px; 
   }

  summary {
  display: block;       
  padding: 6px 6px;
  font-weight: bold;
  font-size: 16px; 
  cursor: pointer;
  color: Green;
  background: #fff;
  border-bottom: 1px solid #ddd;
}
  
    .toc {background:#fff; padding:1em; border:1px solid #ddd; border-radius:8px; margin-bottom:1.5em;}
    .toc ul {list-style:none; padding-left:0;}
    .toc li {margin:0.3em 0;}
    .toc a {text-decoration:none; color:#0ea5a4;}
    .toc a:hover {text-decoration:underline;}
    </style>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>THE SCORED SOCIETY: DUE PROCESS FOR AUTOMATED PREDICTIONS</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <main class="container">
    <h1>THE SCORED SOCIETY: DUE PROCESS FOR AUTOMATED PREDICTIONS</h1>
    <p>作者：Danielle Keats Citron & Frank Pasquale</p>
    <p>引用：Danielle Keats Citron & Frank Pasquale, <em>The Scored Society: Due Process for Automated Predictions</em>, 89 Washington Law Review 1 (2014).</p>
    <hr />

<h2>🗂️大纲</h2>
<h2>📝主要内容</h2>

    <h3>引言：评分社会的介绍</h3>
<p>作者开篇引用小说The Circle描述了一个充满了评分机制的社会。在这种社会中，处处充满了监控和评分，人的一举一动都可能被用于评价他们的财务状况、工作表现、罪犯的再犯可能性等。</p>
<p>存在两种类型的评分机制：<br>(1)是大型机构收集大量的个人活动、行为信息，使用预测性算法来评估个人的状况。<br>(2)是个体之间的评分，一种是单向的，如学生给老师评分；而另一种则是双向的，如在租房平台上，房主可以给租户评分，租户也可以给房主评分，互相成为对方的约束。 </p>
<p>文章指出，目前第一种评分，即强大的机构利用算法对个体进行评估更常见。虽然这些评分机构会把评分描述为一种鼓励勤奋、促进公平的机制，但以下的事例表明，这些评分也常被用于侵害个人。</p>

<details>
  <summary>这些算法评分存在两方面的问题：(1)这些推断可能并不准确，给当事人带来不必要的交易机会、社会评价下降等伤害；(2)虽然有些评分确实可以客观上反映某种大概率的风险，但对个人的隐私、行动自由带来的限制和侵害。</summary>
  <p>在作者列举的事例中，算法基于相关关系作出判断，如基于婚姻咨询而推测其信用风险提升、基于热量摄入多而进行更高的健康保险定价等。</p>
</details>

<p>利用算法进行评分的支持者们通常会宣称算法能够消除人类在评估中的瑕疵、缺陷：自动化系统对所有人使用同一评价标准，因此消除了歧视。但这种宣称是误导性的，因为算法是由数据和代码组成的，人类已有的偏见、程序员的偏见和价值观都会通过数据和代码转化到算法中。并不存在无偏见的算法。</p>
<p>算法的支持者们坚称我们可以信任算法能够自我调整至更高的准确度。如果数据/信息足够全面、算法合理，那么就可能得出较合理的判断。但是现实中的算法是隐秘的，评分者拒绝披露算法的方法和逻辑。我们无法去查看、测试这些算法和数据是否是全面、合理的。而本文后续的论证将表明，这些评分系统事实上的效果是糟糕的。</p>

    <details>
    <summary>算法评分系统的运行不应当脱离专业的监管。</summary>
    <p>大数据驱动下的预测算法具有一定的自发性和不可控性。这种算法可能发展出一种类人智能（AI，也称人工智能）。存在两种意义上的人工智能，分别是工程学上的和认知科学上的。工程学上的人工智能强调结果，即算法能够完成那些需要人类智慧才能完成的任务。这种人工智能不关注过程，是一种典型的“黑箱”。认知科学意义上的AI则强调算法的运行过程、结构设计模仿人脑的运行方式。如果要让评分系统既达到工程目标上的类人智能，又要其保留“公平”的人类价值观，就需要设置人类复核（human review）来保障。</p>
    <p>相关的讨论已经在“杀手机器人”领域展开，其中描述了AI驱动武器中三种人机交互方式：</p>
	<p>（1）人类在环（human-in-the-loop）武器：机器人可以在人类的命令下选择目标并且实施攻击；</p>
	<p>（2）人类在环之上（human-on-the-loop）武器：机器人可以在人类的监管之下选择目标并实施攻击，人类可以否决机器的行动；</p>
	<p>（3）人类不在环（human-out-of-the-loop）武器：机器能够在无人类输入或介入的情况下自主选择目标并实施攻击。</p>
	<p>人权支持者和计算机科学家们认为人类不在环武器系统违反了国际法，因为AI系统无法充分落实“区分规则”（区分战斗人员和非战斗人员）和比例原则。</p>
	<p>正如杀手机器人违反了基本的法律规范，评分系统至少也应被审慎地看待。我们不应当在不理解算法如何得出结果并且确保人类复核员能够对关于公平和准确性的担忧进行回应的情况下就轻易地接受其给出的预测。评分系统通常从工程上来评估，我们呼吁把认知意义上的人工智能融入到评分系统中。
    </details>

  <p>在这篇文章中，我们探索了评分系统判断个体对公平、正义这些人类价值观的影响。尽管算法预测对个人生活机会的伤害总是以武断、歧视的方式进行，但算法仍保持着秘密状态。人类监管应当去监管这些问题。</p>
<p>本文以信用评分系统作为一个案例研究来考察一般性的评分社会的问题。  </p>
  <ul>
    <li>第一部分描述了算法评分系统的发展及其问题。证据表明原本被认为是客观的数据分析事实上是任意武断的，并且会对女性和弱势群体带来不均等的影响。评分系统无法被有意义地核查，因为组成它们的技术被认定为商业秘密。</li>
    <li>第二部分提出评分系统的透明性是必不可少的。这源于我们的正当程序传统，并号召通过“技术性正当程序”把人类的价值观和监管融入算法中。评分系统及其结果必须接受专业审查（expert review）。</li>
</ul>

    <h3>第一部分 对金融风险评分系统的案例研究</h3>
    <h4>A. 信用评分系统历史的简介</h4>
<p>信用评分系统在美国已经发展了60多年。最初，零售商和银行员工评估借款人的“可信度”。当时，专家们被委托作出借款决定。在二战后，专门的金融公司参与到其中。1956年，Fair, Isaac &amp; Co. (FICO)公司发明了一个三位数的信用分数，向银行和金融公司提供服务，，该分数的范围从300到850分。FICO把这个分数作为消费者是否会债务违约的预测。FICO的评分至今仍非常强大，尽管信用机关（消费者报告机构consumer reporting agencies）也开发了他们自己的评分系统。</p>
<p>信用评分系统在信贷证券中发挥了重要作用，金融机构在发放贷款时对信用评分较高的人较低利率，给信用评分较低的人较高利率。并由此开发、交易了许多金融衍生品，最终引爆了2008年的次贷危机。这种根据评分来决定价格的机制存在黑暗的一面。信用评分系统的用途发生了变化：最初用于决定是否发放贷款，拒绝给信用低的人发放贷款，是一种屏蔽风险的手段；之后用于决定贷款的利率，信用低的人利率高；这是一种风险管理措施，会引发一定的风险。次贷危机是由于给大量低信用的人发放贷款产生了泡沫。</p>

    <h4>B.信用评分的问题</h4>
<p>信用评分系统存在三个基本问题：不透明、武断的结果、对女性和少数群体的不均等影响。</p>
<p><b>1、不透明</b></p>
<p>评分系统的过程无法被个体或监管者完全理解、质疑挑战或审计。根据已有的案例，外部无法对评分系统背后的预测算法进行审计。算法，甚至是评分的中位数、平均数都仍是秘密的。这种透明度的缺失导致消费者无法理解他们的分数为什么以及会如何变化，FICO以及征信机构并未解释个人的行为对特定类别的影响程度。消费者无法确定最优的评分行为或者该做什么来避免分数下降。已有的一些披露不足以让人们预测自己行为的后果。对此，还专门有一些书籍、文章、网站研究如何提高征信分数。</p>
<p><b>2、评估的武断性</b></p>
<p>评分系统给出的结果可能是任意、武断的。在一个研究中，不同机构对同一个人的信用评分差异较大。其次，评分系统还被用于操控和压制当事人，在一场要求征信机构披露信息的运动中，参与其中的个体的信用评分被下调。评分系统的用途突破了正常、合理的范围。此外，评分系统还成为了歧视、偏见的来源。量化的评分系统看似精准公平，但其使用同一套规则来应对所有情况，没有考虑地域、文化等个性化差异等因素，因此其并不能准确反映真实的风险。算法“精准”的表象造成了人们对算法错误的信赖。</p>
<p><b>3、不均衡的影响</b></p>
<p>算法系统不仅没有消除（人类）现有的歧视，反而把这些歧视隐蔽化、系统化了。这是由于两个方面的原因：一是算法的开发者的偏见和价值观会通过代码带入算法中；二是训练数据中包含着历史性偏见。已有外部证据表明，这些偏见会对传统上处于弱势地位的群体产生不均衡的负面影响。这种对弱势群体的影响会形成恶性循环，弱势群体被不利评价后会陷入更难的境地，进而在未来的评分中继续处于不利境地。为了应对这些问题，就需要访问算法的源代码、开发者的笔记来测试是否存在偏见。在Allstate案中，和解协议的一部分就是允许原告的专家评判和改进评分模型。</p>
<p>ECOA法（Equal Credit Opportunity Act）禁止在借贷中歧视，Regulation B把ECOA法适用于信用评分系统。Regulation B要求否定评分或借贷的需要给出与此相关理由，并准确描述那些评分者所依据的因素（factors）。但由于执法活动较少，ECOA的有效性难以评估。一方面因为诉讼成本较高，通常超过算法对个体造成的损害；另一方面罚款较低，不足以威慑此类行为。</p>

    <h4>C.现有监管模型的失效</h4>
<p>不可靠的信用历史记录推动立法者监管信用产业。1970年。国会通过了《公平信用报告法案》（Fair Credit Reporting Act, FCRA），其担忧日益增长的个人信息数据库会被用于隐秘的、对消费者有害的用途。FCRA是美国第一个信息隐私立法。FCRA强制：（1）征信机构以及所有其他消费者报告机构确保信用历史记录是准确并相关的。（2）消费者有权利查看他们的信用记录，（3）要求更正，（4）以及给他们的信用记录做注释，如果争议无法解决。评分机构对FCRA的遵守可以豁免诽谤法。</p>
<p>对信用评分，2003年通过的Fair and Accurate Credit Transactions Act (FACTA)要求征信机构向个人有偿披露信用评分，该费用由FTC规定。但FACTA并不要求消费者报告机构向消费者披露任何关于评分的信息，除了信用决策中的四个关键因素。可惜的是，这四个因素对评分的解释很有限。高分和低分者都可能有相同的因素，该法并不要求评分者告知个人这些因素的权重、对特定分数的影响程度。也就是说，这些披露的因素无法对个体行动提供具体的指导。该行业仍然高度不透明，被评分的个体无法确定他们行为的确切后果。</p>
<p>尽管FCRA向个人提供争辩他们信用历史的机会，但其并不要求征信机构披露他们是如何把历史记录转化为分数。这被视为交易秘密。尽管有这些隐秘性，我们仍然可以对信用评分建构起的黑箱社会得出一些结论。我们已经看到一些证据表明信用评分所产生的武断结果可能会事实上强化不平等。现在，我们转向我们的提议，通过程序（过程）监管来监督我们的评分社会，同时兼顾包括开发者的知识产权在内的其他价值。</p>

    <h3>第二部分 对自动化评分系统的过程性（procedural）保护措施</h3>
    <p>预测性的评分是信息时代的既定事实，但其不应该缺少核查。有意义的可问责性是必要的。对于那些会对个人生活机会有重要影响的预测性算法，如借钱、工作、旅行、获得房屋、入学等机会，过程监管是非常必要的。</p>
<p>关键的是，评价性的算法不仅仅是在描述、预测社会/个体的现状，还在主动地影响和塑造着个体（社会）的未来。用什么标准去评价人，人就会被塑造成什么样。因此，<b>算法不仅仅是描述性的，而是规范性的</b>，这就会涉及道德上的正当性和合理性问题。鉴于评分系统在人们生活中占据核心地位，因此应满足公平性要求。</p>
    
    <div style="border: 2px solid #333; background: #f0f8ff; padding: 16px;">
    <p><b><span style="color:red;"><i>本文认为，市场化运作并不能解决本文提到的评分侵害问题，并论证了对市场化机构适用公法正当程序要求的合理性：</i></span></b></p>
    <p>目前评分系统都是市场化运作，有观点认为市场机制能够调整这其中涉及的风险、激励评分机构做出较好的服务。但本文作者提出了两点不同的看法：（1）市场机制的运行依赖关于评分公司的信息，这些评分公司为消费者提供评融资、进行排序以及评级。因此这些评分公司的活动应当得到适度披露。（2）这些私人评分系统事实上在挥舞着类似政府性质的权力（他们能够决定哪些人值得获得资金支持；这套评价标准在主动塑造着人/社会），因此其应遵守高于普通企业的标准。</p>
<p>法治建设的最大成就之一是使得统治者对其决定负责，并赋予个体基本权利。新的算法决策者就是这种新的统治者，统治着个人生活中许多重要的方面。如果法律和正当程序在这个领域缺失，我们就会步入一个声誉中介不受问责的封建秩序。</p>
<p>这并非完全把公法中的正当程序保障迁移到信用评价机构上，因为毕竟信用评价机构并非公权力主体。而是依据正当程序的理念，结合评分机构对个人生活影响的程度，来设置程序性保护。正当程序底层的价值包括：透明、准确、可问责性、参与和公正。</p>
    </div><br>
<div style="border: 2px solid #333; background: #f0f8ff; padding: 16px;">
  <p><b><span style="color:Green;"><i>评分算法中正当程序实现的方式</i></span></b></p>
<p>可以通过技术性正当程序来实现可问责性。技术性正当程序设置了一些程序来确保预测算法受到检查review和复核revision，以确保其公正性和准确性。程序性保护要覆盖两个环节：（1）算法开发阶段（相当于传统的立法）；（2）个案决策阶段（相当于传统的执法）。</p>
<p>具体而言，通过两方面来实现程序性保护。（1）联邦监管者FTC能够获得这些评分系统的完全访问权限，并能够检查其是否符合公平的要求。（2）个体在个体决策中能够有权利/参与。</p>
</div>
    
    <h4>A. 监管者的审查/监督oversight</h4>
<p>作者根据评分系统运作的不同阶段分别设置了不同监管以及个体权利。</p>
    <ul>
      <li><details><summary>1）收集关于被评分个体的数据；</summary><p>应按照FCRA的规定，个体应有权利检查、更正以及争辩不准确的数据，以及指导数据的来源。但现在数据经纪商拒绝透露数据来源，因为其属于保密协议的一部分。</p></details></li>
      <li><details><summary>2）把收集到的数据计算得到评分；</summary><p>理想情况下，计算的过程应该是公开、可被检查的。在商业秘密trade secret需要被保护时，应仅允许专门的、非公开审查。改变默认保密的现状，转向人们理应知道自身如何被评分的消费者预期。</p></details></li>
      <li><details><summary>3）把评分结果分发给决策者，例如雇主；</summary><p>由于评分者选择向谁分发评分受到第一修正案言论的保护，法律不能对分发行为进行限制。只能要求评分者将向谁分发告知消费者。这种通知消费者的要求只会增加言论，而不会限制或压制/审查言论，因此不会与第一修正案冲突。（<span style="color:yellow;">现实中的分发可能是大批量的，要求全部通知到个体，可能会给评分者带来较高的成本，这种成本是否会被视为是对言论的限制？</span>）</p></details></li>
      <li><details><summary>4）雇主或其他决策者利用评分进行决策。</summary><p>这是最具争议的阶段，作者认为评分系统应当受到资质/许可licensing和审计要求的约束，当涉及重要的、关键的领域，如雇佣、保险、健康照料。这中许可/资质批准可以由已经获得EEOC、OSHA或者劳工部许可的私营实体来完成（官方许可一些私营的审批机构，这些审批机构再对评分机构进行许可）。资质/许可模式在健康信息技术领域已被证明是有用的。</p></details></li>
    </ul>

<p>作者认为，不能指望这些评分机构会自我约束，因为企业仅负有找到最高效处理模式的义务，并无义务维护公平等其他社会价值。许可制度可作为一种保障机制，确保公共价值能融入该技术。</p>
<p>许可主体能够确保特定的敏感数据不会被用于评分。对于其他存在争议的数据类型，可召开公开听证会以收集意见，判断其是否应被用于决策。</p>
<p>上述提出的多项建议都需要立法来支撑。但我们清醒地认识到，国会目前并无意愿推进这些建议。但时刻准备这些政策建议至关重要，因为在发生突发事件或快速变革时，这些建议就能够快速应对和付诸实施。</p>
<p>幸运的是，联邦贸易委员会（FTC）拥有法定权力，可以不通过立法而推进上述的多个建议。FTC可以依据《联邦贸易委员会法》第5条赋予的打击“不公平”贸易行为的权力对信用评分系统进行监管。监管实践可能成为未来立法/行业共识的范本。“不公平”商业行为指的是对消费者造成实质损害或有造成实质损害之虞、消费者无法合理规避，且损害大于利益的行为。2008年，联邦贸易委员会（FTC）曾依据该权力对一家信贷机构采取行动——该机构依据未公开的行为评分模型削减消费者信贷额度，该模型会因消费者将信用卡用于个人咨询等特定交易而对其进行惩罚。FTC近年来在持续并且越来越关注私人部门使用算法评分对个人做出决策的情况。page 23-24。</p>
<p>本文认为，FTC应该进一步采取如下措施来实现监管：</p>
	<h5>1、辅助测试的透明度</h5>
<p>FTC应当能够访问那些会不公平地伤害消费者的评分系统。具体而言，FTC可访问的程度以及审计的频率应当与评分系统表现出的不公平程度相称。特别应当关注那些个体无力保护自己的场景。FTC的技术专家应当能够测试评分系统中是否含有偏见、武断和不公平的错误分类。为此，FTC专家需要查看数据库（包含着个人信息）、源代码、程序员的笔记/注释（描述各种变量、关联性、推理）。</p>
<p>在技术快速变化的时代，为了确保这种核查review是有意义的（避免刚核查完技术就发生迭代了），FTC的技术专家应当能够对评分系统进行有意义的访问。他们应当能够测试系统来检测与分类有关模式和关联性是否会触犯美国法已经规定的内容，如种族、国籍、性取向、性别，评分系统应通过测试套件进行检验，此类测试套件包含由政策专家设计的预期及非预期假设场景。测试既是软件开发的固有规范，也有助于发现程序员可能存在的偏差，以及人工智能系统演化过程中产生的偏差。（通过外部测试来核查算法）</p>
<h5>2、风险评估报告和建议</h5>
<p>FTC在完成测试后，应当给出一个《隐私与公民自由影响评估报告》，该报告分析评估了评分系统对被保护群体的负面、不均等影响、结果的任意性、错误分类以及隐私损害。FTC还应对每一项给出适当的风险缓解措施。</p>
<p>一个重要/争议性的问题是，公众能够在多大程度上访问数据集以及了解预测算法的逻辑。本文认为，每个个体应当能够访问与其相关的所有数据。理想状况下，预测算法的逻辑也应当向公众公开，接受审查。作者的理由是，那些支持保密的理由的证据不够充分：（1）公开会削弱创新。本文认为，依赖此类系统的放贷机构希望避免违约——这一点本身就足以激励其维护和改进这类系统。即便公开，信贷机构也有足够的商业动机去持续开发和改进这些算法。简言之，开发者类算法的动机并非知识产权保护所带来的激励。另一方面，作为大数据支持的行业，评分机构真正的竞争力体现在能够收集到大量的数据。在个案中向个体披露个人信息和算法逻辑并不会削弱这种竞争力。（2）公开可能引发算法博弈（算计算法）。本文认为，在信用评估中伪造记录/专门钻空子的成本较高。（例如为了达到一定的信用评级，可能需要满足一些经济指标，具备一定的消费能力、储蓄、职业等，这些是难以通过简单投机来实现的）</p>
<p>更进一步的，评分系统的源代码、算法预测结果及建模过程是否应向受影响的个人、乃至最终向广大公众公开？反对的理由认为，鉴于高度敏感的知识产权、国家安全相关的担忧，应当对大数据在一定程度上保密。但本文认为，无处不在、不透明且自动化的评分系统对人格尊严构成了威胁，这一威胁远超过上述担忧的重要性。（公开的必要性大于公开的代价）</p>
<p>本文认为，至少，当预测评分损害个人获取信贷、工作、住房及其他重要机会的能力时，个人（1）应获得有效的告知，（2）并拥有对该评分提出异议的机会。</p>
<p>我们还应当确保学者和专家能够对这些评分系统发表意见。有学者认为，公众对这些评分系统发表意见非常重要，特别是对影响评估这类措施。并且，Tal Zarsky认为，“提供有关分析中所使用数据及数据库的种类和形式的信息……仅会产生有限的社会风险……[通常仅存在于]涉密政府数据集的情形中。”</p>

    <h4>B. 对个体的保护</h4>
<p>在评分场景下建构技术性正当程序，可以参考政府对个体做出不利决策时的告知要求（Notice）。</p>
<p>在正当程序原理下，告知必须是合理设计的，以使个体了解政府对他们做出的不利决定。告知的充分性要求告知个体四个方面的内容：（1）做出决定的事项issue；（2）支持政府决定的证据evidence；（3）机构做出决策的流程/程序process。清晰告知的意义在于能够降低决策机构基于不准确或误导性的事实前提、或错误适用规则的可能性。（清晰告知的作用在于防范/减轻错误）</p>
<p>告知不足是自动化决策系统的经典争议，已有不少案例争议就是由于自动化决策系统未能告知相对人、给出合理解释引发的。本文认为，告知不足的原因在于算法开发者没有设置审计追踪功能，以记录支撑计算机每一项决策的事实与法律依据。技术正当程序要求自动化系统必须包含不可变更的审计追踪功能，以确保个人能获知对其不利的决策所依据的理由。（要实现有效告知，大致需要几个环节：1、计算机系统记录的决策的依据，也就是本文这里提到的，有可告知的内容；2、决策者能够将这些内容通过有效的途径、以有效的方式告知相对人，让相对人能够理解。这里似乎只讨论了第一步）</p>
<p>（基于下文的讨论，算法系统的“告知”更多是对传统告知“精神”和“目标”的传承，在实现方式方面已经不是传统的“直接告诉相对人”的方式了，更多通过其他措施来实现告知的价值。如专家审查、交互式指引等。）</p>
<p><b>1、以审计追踪来确保告知</b></p>
<p>受到不利影响的消费者能够得到合理的告知，如果评分系统包含记录了预测过程中算法做出的相互关系correlations以及推理（推论）（inference）的审计追踪功能。通过审计追踪的记录，个体能够有途径理解他们的评分。他们能够质疑/挑战不恰当的描述和错误的推论。</p>
<p>即便评分者成功地将他们的所有的代码以及算法向公众维持了秘密状态，让独立第三方来复核该算法仍然是可能的。一种可能的方式是在任何个案裁判中（individual adjudication），算法系统的技术方面可以被保护令要求保持机密来保护。（这里应该指的是涉及算法评分系统的司法纠纷的裁判）</p>
<p>另一种可能的方式是将算法系统的披露仅限定在受信任的中立专家。这些专家能够被委托访问审计追踪中记录的相关关系和推理。他们能够访问，如果评分是基于非法的分类特征，如种族、国籍或性别或不恰当的描述。这种路径能够同时保护评分者的知识产权和个体的权益。</p>
    <p><span style="color:Green;">（审计追踪是基础，也是绝对必要的。关于审计追踪的内容的访问权限/个人如何来访问，这里认为个体不能直接访问，只能通过1、在法院保护令的约束下访问；2、通过专家来访问，记录只披露给中立的专家，由专家来审查评估）</span></p>
<p><b>2、交互模式Interactive Modeling</b></p>
<p><span style="color:red;">（个人总结为“面向未来的解释”：不纠结过去是怎么回事，而是为未来的改变、提升评分提供指引）</span></p>
<p>另一种方式是向消费者提供机会，来测试如果他们的信用记录发生变化，他们的分数会发生何种改变。想象这样一个交互界面：个人信用记录的各个方面都在一个维基（wiki）页面上呈现。并且该界面会告诉消费者采取不同的行动/措施会对评分有多大的影响（提升/降低多少分数）。（这种交互界面能够给消费者提供清晰、可预期的指引，帮助消费者了解特定行为的预期后果，并有助于消费者改善提升评分。）</p>
<p>被称为“反馈与控制”的交互模式，已在“设计中嵌入价值”运动的推动下，成功应用于其他技术领域。这种模式促使自动化系统能让个人更清晰地感知下一步的行动会如何影响对自己的评分。例如，加拿大移民局允许个人在永久居民资格的初步“测试”中输入各种情景。这份公开评分标准并不保证任何人都能获批，且会随时间修订。尽管如此，它仍大致勾勒出了评分过程中（1）哪些因素重要，以及（2）重要程度如何。</p>
<p>（从审计追踪记录的数据到这里的评分预测、指引，中间需要一个转化的步骤，有点类似算法解释，这个工作由谁来完成？设置专门的人来做还是可以由算法来完成？）</p>
<p>但上述方案的实现也存在一定困难。征信机构确实需要一些灵活性flexibility来评估一个快速变化的金融环境。任何给定的评分或许都给予数以百计的、持续变化的变量。在经济下行期，逾期的严重性会小于经济上行期，因为大家普遍都逾期。征信机构或许没有能力对某个行为在未来一周、一个月或一年对评分的确切影响。（评分系统实际上在不断变化，以适应新的环境）不过，他们也可以使用旧算法给出一个预测。（有一种预测/指引总比没有强。）</p>
<p>（困难的总结：1、评分依据是成百上千的、相互之间复杂关联的，把这些转化为清晰的行为指引，可能存在困难；2、评分依据可能是非结构化的、微小的行为数据，并且多个行为累加可能才会引发评分的改变，消费者的一个行为可能不会对评分产生太大的影响；3、宏观社会环境在变化，相同的历史数据在不同时期可能会被不同解读，即评分系统的逻辑也在变化，难以预测未来评分系统会如何看待该行为。这里是否还有保护信赖利益的问题？如果算法系统给出了某种预测，那么将来系统发生变化，对该行为的预测是否要保持不变？）</p>
<p>为实现上述目标，我们需要使用创新的方法来监管（金融、保险以及实体产业中应用的）评分系统。甚至可能考虑设置一个公共、公开（public）的征信机构。即便刚开始是一种实验性质的，但这也有好处：如果一个公共、公开的评分系统能够像私人的评分系统一样好，那么这就能够削弱传统上要求保持评分系统秘密状态的理由。关于这些理由的讨论，在下一部分将深入。</p>

    <h4>C. 对可能的反对理由的回应</h4>
<p><b>1、反对理由一：算计算法的担忧</b></p>
<p>征信机构会反对上述的透明度要求，声称任何形式的透明度要求都会动摇信用评分存在的根本原因。（公开了，信用评分就失去意义/准确性了，认为秘密性是保持信用评分机制有意义的前提。偷偷观察到的，才是当事人最真实的状态）个体就可能算计评分系统，如果关于评分算法的信息被公开或由于违反保护令而被泄漏。被评分的消费者将获得作弊的“武器”，隐藏高风险行为，并规避机构对欺诈等问题的合理关切。</p>
<p>我们承认，如果所有人都知晓那些间接体现良好信用的指标，这些指标的预测效力可能会大幅减弱。例如，倘若“最优信用账户数量为4个”这一信息被广泛知晓，那些急需贷款的人或许最有可能调整自身财务状况，以迎合这一标准。</p>
<p>但本文认为，<span style="color:Green;"><b>保持秘密状态的代价要高于收益</b></span>，具体回应如下：</p>
<p><b>回应一：对社会、个体的负面影响</b></p>
<p>【给社会带来风险】然而，作为一个社会，我们也应当反思：这种通过隐秘、全景式的方式对人进行评判和分类的做法，是否恰当？它已然成为美国历史上最严重的金融危机之一的推手——通过声称能以极高精度对个人信用资质进行科学评级，为大规模的次级贷款提供了合法性。隐秘的信用评分可能会毫无必要地复杂化社会运行，为危险的投资行为披上客观的外衣，并将歧视性做法嵌入难以破解的算法之中。</p>
<p>【不利于创新】信用评分的保密性还可能阻碍渐进式创新：如果外部人员无法接触到现有的评分系统，又怎能开发出更优的评分系统呢？</p>
<p>【对个体的侵害】隐秘的信用评分会损害公共利益，因为不透明的评分方法使得那些感觉（且很可能确实）受到不公对待的人难以申诉维权。</p>
<p><b>回应二：算计算法等理由并不能对抗外部专家</b></p>
<p>退一步说，若评分机构能提供证据证明公开（评分信息）会产生不良影响，那么这或许能成为不向公众公开评分算法的正当理由。但该逻辑并不适用于联邦贸易委员会（FTC）或第三方专家——他们负有对专有信息保密的义务。</p>
<p><b>2、可能的批评之二：上述措施无法应对非信用类以及不面向公众的评分</b></p>
<p>另一个可能的批评是，我们的上述提议措施仅在评分系统的存在本身为公众所知的情况下才奏效，例如信用评分领域。在非信用场景中，机构并无法律上向公众及受影响个人披露评分系统的义务。</p>
<p>我们前文所讨论、公众所看到的那些评分系统之所以不是处于秘密状态，是因为其商业模式便是向公私实体出售评分。而另一些商业模式下，我们是察觉不到评分系统的存在的。例如数据经纪商基于非信用相关标准对消费者进行评级、分类和打分，以此规避《公平信用报告法》（FCRA）规定的义务。</p>
<p><b>本文的回应：FTC有权监管，以对个人有重大影响的决策为切入口</b></p>
<p>毫无疑问的是，消费者不可能对一个其根本不知道其存在的评分系统提出质疑。即便未被纳入FCRA的监管范围，那些关于个人健康、就业能力、生活习惯等方面的隐秘评分，也可能构成不公平的行为。在这种情况下，联邦贸易委员会（FTC）有权要求相关机构披露这些隐秘的评分系统。（FTC的权力具有兜底性质）</p>
<p>当然，对于那些始终处于保密状态的评分系统，联邦贸易委员会（FTC）也难以发现并展开调查。立法者可强制要求那些影响重要人生机遇的评分系统保持透明。（立法/监管的切入口是从对个体有重大影响的角度去追溯、揭露这些秘密状态的评分系统）例如，加利福尼亚州一直走在推动企业提高消费者信息使用透明度的前沿。[161] 联邦贸易委员会也曾呼吁联邦立法者通过相关法案，赋予消费者获取数据经纪商所持有的关于其个人信息的权利。[162] 2013年9月，参议院商业委员会主席杰伊·洛克菲勒（Jay Rockefeller）宣布其委员会将对顶级数据经纪商的信息收集与共享行为展开调查。[163] 我们尤其支持此类举措——只有当评分系统为公众所知且能接受质疑时，对其进行有意义的评估才成为可能。</p>

    <h3>结论</h3>
<p>一个充满着评分的社会可能会产生连锁反应，形成恶性循环，即犯一个错误可能会导致后续持续的失败而无法获得改变的机会。正如Wolff和De-Shalit所说，“若缺乏类似本文所提出的行动方案，社会注定会持续强化根深蒂固的特权与不利地位，拉大贫富差距，并使劣势地位不断延续”。Michael Walzer的社会理论也提供了一种有力的反对大数据随意混搭各类数据源以剥夺机会的做法，提供了极具说服力的论据。评分系统具有强大的吸引力——其简洁性营造出精确且可靠的假象。但预测算法绝非一定准确或公平，它们可能以武断且带有歧视性的方式，压缩人们的人生机遇。</p>
<p>因此，监督这些可能引发恶性循环的评分系统，应成为法律体系的关键目标之一。作为一个社会，我们已作出承诺保护消费者免受其无力防范的严重伤害。我们也致力于让个人获知针对其作出的重要决策，并拥有提出异议的机会。这些承诺能帮助我们构建评分系统的正当程序模型。</p>
<p>透明度是至关重要的第一步。（1）首先应向联邦贸易委员会（FTC）公开，使其能依据自身监管“不公平行为”的职权对评分系统展开调查。</p>
<p>（2）向个人或代表他们的中立专家开放“黑箱”式的评分系统，是允许他们对“算法的武断性”提出质疑的关键。</p>
<p>我们的建议虽属初步构想，但仍希望联邦贸易委员会及相关立法者能采取行动，为这个“被评分的社会”引入程序规范性与监督机制。</p>
<p>（透明度是应对算法问题的第一步，是后续活动开展的前提，对算法的监管治理不能止于透明度。）</p>
</body>
</html>
