
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>THE SCORED SOCIETY: DUE PROCESS FOR AUTOMATED PREDICTIONS</title>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <main class="container">
    <h1>论文编译——THE SCORED SOCIETY: DUE PROCESS FOR AUTOMATED PREDICTIONS</h1>
    <p><strong>作者：</strong>Danielle Keats Citron & Frank Pasquale</p>
    <p>引用：Danielle Keats Citron & Frank Pasquale, <em>The Scored Society: Due Process for Automated Predictions</em>, 89 Washington Law Review 1 (2014).</p>
    <hr />

    <h2>🗂️大纲</h2>
    <h2>📝主要内容</h2>
	<h3>介绍 评分社会的介绍</h3>
     <ul>作者开篇引用小说The Circle描述了一个充满了评分机制的社会。在这种社会中，处处充满了监控和评分，人的一举一动都可能被用于评价他们的财务状况、工作表现、罪犯的再犯可能性等。</ul>
     <ul>存在两种类型的评分机制：一种是大型机构收集大量的个人活动、行为信息，使用预测性算法来评估个人的状况。另一种机制是个体之间的评分，一种是单向的，如学生给老师评分；而另一种则是双向的，如在租房平台上，房主可以给租户评分，租户也可以给房主评分，互相成为对方的约束。</ul>     
     <ul>作者指出，目前第一种评分，即强大的机构利用算法对个体进行评估更常见。虽然这些评分机构会把评分描述为一种鼓励勤奋、促进公平的机制，但以下的事例表明，这些评分也常被用于侵害个人。p.4在作者列举的事例中，算法基于相关关系作出判断，如基于婚姻咨询而推测其信用风险提升、基于热量摄入多而进行更高的健康保险定价等。</ul>
     <ul>利用算法进行评分的支持者们通常会宣称算法能够消除人类在评估中的瑕疵、缺陷：自动化系统对所有人使用同一评价标准，因此消除了歧视。但这种宣称是误导性的，因为算法是由数据和代码组成的，人类已有的偏见、程序员的偏见和价值观都会通过数据和代码转化到算法中。并不存在无偏见的算法。 </ul>
	
	<h3>第一部分 对金融风险评分系统的案例研究</h3>
	  <h4>A. 信用评分系统历史的简介</h4>
	   	<ul>信用评分系统在美国已经发展了60多年。最初，零售商和银行员工评估借款人的“可信度”。当时，专家们被委托作出借款决定。在二战后，专门的金融公司参与到其中。1956年，Fair, Isaac & Co. (FICO)公司发明了一个三位数的信用分数，向银行和金融公司提供服务，该分数的范围从300到850分。FICO把这个分数作为消费者是否会债务违约的预测。FICO的评分至今仍非常强大，尽管信用机关（消费者报告机构consumer reporting agencies）也开发了他们自己的评分系统。</ul>
	   	<ul>信用评分系统在信贷证券中发挥了重要作用，金融机构在发放贷款时对信用评分较高的人较低利率，给信用评分较低的人较高利率。并由此开发、交易了许多金融衍生品，最终引爆了2008年的次贷危机。这种根据评分来决定价格的机制存在黑暗的一面。信用评分系统的用途发生了变化：最初用于决定是否发放贷款，拒绝给信用低的人发放贷款，是一种屏蔽风险的手段；之后用于决定贷款的利率，信用低的人利率高；这是一种风险管理措施，会引发一定的风险。次贷危机是由于给大量低信用的人发放贷款产生了泡沫。</ul>
	  <h4>B.信用评分的问题</h4>
	  	<ul>信用评分系统存在三个基本问题：不透明、武断的结果、对女性和少数群体的不均等影响。</ul>
	  	<h5>1、不透明</h5>
	  		<ul>评分系统的过程无法被个体或监管者完全理解、质疑挑战或审计。根据已有的案例，外部无法对评分系统背后的预测算法进行审计。算法，甚至是评分的中位数、平均数都仍是秘密的。这种透明度的缺失导致消费者无法理解他们的分数为什么以及会如何变化，FICO以及征信机构并未解释个人的行为对特定类别的影响程度。消费者无法确定最优的评分行为或者该做什么来避免分数下降。已有的一些披露不足以让人们预测自己行为的后果。对此，还专门有一些书籍、文章、网站研究如何提高征信分数。</ul>
	  	<h5>2、评估的武断性</h5>
	  		<ul>评分系统给出的结果可能是任意、武断的。在一个研究中，不同机构对同一个人的信用评分差异较大。其次，评分系统还被用于操控和压制当事人，在一场要求信用机构披露信息的运动中，参与其中的个体的信用评分被下调。评分系统的用途突破了正常、合理的范围。此外，评分系统还成为了歧视、偏见的来源。量化的评分系统看似精准公平，但其使用同一套规则来应对所有情况，没有考虑地域、文化等个性化差异等因素，因此其并不能准确反映真实的风险。算法“精准”的表象造成了人们对算法错误的信赖。</ul>
	 	<h5>3、不均衡的影响</h5>
	  		<ul>算法系统不仅没有消除（人类）现有的歧视，反而把这些歧视隐蔽化、系统化了。这是由于两个方面的原因：一是算法的开发者的偏见和价值观会通过代码带入算法中；二是训练数据中包含着历史性偏见。已有外部证据表明，这些偏见会对传统上处于弱势地位的群体产生不均衡的负面影响。这种对弱势群体的影响会形成恶性循环，弱势群体被不利评价后会陷入更难的境地，进而在未来的评分中继续处于不利境地。为了应对这些问题，就需要访问算法的源代码、开发者的笔记来测试是否存在偏见。在Allstate案中，和解协议的一部分就是允许原告的专家评判和改进评分模型。</ul>
	  		<ul>ECOA法（Equal Credit Opportunity Act）禁止在借贷中歧视，Regulation B把ECOA法适用于信用评分系统。[ Regulation B规定了信用评分系统中不得使用的特定数据，例如：公共救济状态、任何人生育或抚养子女的可能性、电话号码登记信息、基于禁止性事由获得的收入、不准确的信用记录、针对已婚与未婚人士的不同标准，以及种族、肤色、宗教、国籍和性别。12 C.F.R. § 202.5 (2013).]Regulation B要求否定评分或借贷的需要给出与此相关理由，并准确描述那些评分者所依据的因素（factors）。但由于执法活动较少，ECOA的有效性难以评估。一方面因为诉讼成本较高，通常超过算法对个体造成的损害；另一方面罚款较低，不足以威慑此类行为。</ul>
	  <h4>C.现有监管模型的失效</h4>
	  	<ul>不可靠的信用历史记录推动立法者监管信用产业。1970年。国会通过了《公平信用报告法案》（Fair Credit Reporting Act, FCRA），其担忧日益增长的个人信息数据库会被用于隐秘的、对消费者有害的用途。FCRA是美国第一个信息隐私立法。FCRA强制：（1）征信机构以及所有其他消费者报告机构确保信用历史记录是准确并相关的。（2）消费者有权利查看他们的信用记录，（3）要求更正，（4）以及给他们的信用记录做注释，如果争议无法解决。评分机构对FCRA的遵守可以豁免诽谤法。</ul>
	  	<ul>对信用评分，2003年通过的Fair and Accurate Credit Transactions Act (FACTA)要求征信机构向个人有偿披露信用评分，该费用由FTC规定。但FACTA并不要求消费者报告机构向消费者披露任何关于评分的信息，除了信用决策中的四个关键因素。可惜的是，这四个因素对评分的解释很有限。高分和低分者都可能有相同的因素，该法并不要求评分者告知个人这些因素的权重、对特定分数的影响程度。也就是说，这些披露的因素无法对个体行动提供具体的指导。该行业仍然高度不透明，被评分的个体无法确定他们行为的确切后果。</ul>
     	<ul>尽管FCRA向个人提供争辩他们信用历史的机会，但其并不要求征信机构披露他们是如何把历史记录转化为分数。这被视为交易秘密。尽管有这些隐秘性，我们仍然可以对信用评分建构起的黑箱社会得出一些结论。我们已经看到一些证据表明信用评分所产生的武断结果可能会事实上强化不平等。现在，我们转向我们的提议，通过程序（过程）监管来监督我们的评分社会，同时兼顾包括开发者的知识产权在内的其他价值。</ul>
	
	<h3>第二部分 对自动化评分系统的过程性（procedural）保护措施</h3>
	  
    <h3>📖规范性研究</h3>
    <h4>·法院使用AI后，会对获得司法公正的机会产生哪些影响？access to justice</h4>
    <ul>
      <li>AI的高昂成本可能会加剧穷富之间获得法律服务的差距？或许人类律师市场也是如此。是加剧了还是缓解了？</li>
      <ul>
        <li>认为缓解了：<em>Chief Justice Robots</em></li>
        <li>认为可能会加剧：<em>Access to A.I. Justice: Avoiding an Inequitable Two-Tiered System of Legal Services</em>, 24 Yale J.L. & Tech. 150 (2022).</li>
      </ul>
    </ul>
      
   	<h4>·AI与法治rule of law的关系</h4>
      <ul>
        <li>从人类在司法中的独特作用角度讨论：<em>Artificial Intelligence, Legal Change, and Separation of Powers</em></li>
        <li>法律的演进：法律在随着社会的实际需要、价值观念等变化，法官在此中发挥发展法律的作用，AI是否能够做到？</li>
         <ul>
      <li>卡多佐：《司法过程的性质》<em>The Nature of the Judical Process</em></li>
      <li>卡多佐：《法律的成长》<em>The Growth of the Law</em> </li>
         </ul>
        <li>司法在分权体系下的功能：AI法官能否承担起制约立法、行政权的功能？</li>
	<li>依赖AI对人类法律思维的削弱，人类未来是否还有能力来思考、判断法律问题？法治的实现依赖于人们的法治思维。<li>
      </ul>
   

    <h3>💡 想法</h3>
    <ul>
      <li>司法场域下使用 AI 的问题清单</li>
    </ul>

    <hr />

    <h2>🧾文献清单</h2>

    <h3>📖 专著</h3>
    <ul>
      <li>卡多佐：《司法过程的性质》<em>The Nature of the Judical Process</em></li>
      <li>卡多佐：《法律的成长》<em>The Growth of the Law</em> </li>
      <li>富兰克林·福尔：《没有思想的世界：科技巨头对独立思考的威胁》<em>World Without Mind: The Existential Threat of Big Tech</em></li>
    </ul>

    <h3>📑 论文</h3>
    <ul>
      <li> Eugene Volokh, <em>Chief Justice Robots</em>, 68 Duke Law Journal 1135, p.1135-1192 (2019).</li>
      <li>Andrew C. Michaels, <em>Artificial Intelligence, Legal Change, and Separation of Powers</em>, 88 U. Cin. L. Rev. 1083 (2020).</li>
      <li>Richard M. Re & Alicia Solow-Niederman, <em>Developing Artificially Intelligent Justice</em>, 22 Stanford Technology Law Review 242 (2019).</li>
      
    </ul>

    <hr />

    <h2>📌 下周工作计划</h2>

    <h3>📖 阅读</h3>
    <ul>
      <li>阅读《司法过程的性质》</li>
      <li>阅读《法律的成长》</li>
    </ul>

    <h3>📝 笔记</h3>
    <ul>
      <li>整理 <em>Artificial Intelligence, Legal Change, and Separation of Powers</em></li>
      <li>整理 <em>Chief Justice Robots</em></li>
    </ul>

    <h3>💡 想法</h3>
    <p>（此处为空）</p>

    <hr />

    <h2>❗ 问题与思考</h2>
    <ul>
      <li>思考人类的判断/交互与法律发展/法律与人类社会的互动、司法权力正当性的关系</li>
      <li>司法权力正当性来源</li>
    </ul>

    <hr />
    <p><a href="../index.html">← 返回主页</a></p>
  </main>
</body>
</html>
